{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pip"
      ],
      "metadata": {
        "id": "BmNi09ugEJGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e41oSpy2Dw2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6579ebf8-ca63-405f-a5ad-192d0083de86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "4q_rusnnEE2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "iTesZffCEojB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "import psutil\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import mode\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, decoders, processors\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, Dataset, IterableDataset, DataLoader"
      ],
      "metadata": {
        "id": "SdiamMUaEQzu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "shell = get_ipython()\n",
        "\n",
        "def adjust_font_size():\n",
        "  display(HTML('''<style>\n",
        "    body {\n",
        "      font-size: 24px;\n",
        "    }\n",
        "  '''))\n",
        "\n",
        "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
        "  shell.events.register('pre_execute', adjust_font_size)"
      ],
      "metadata": {
        "id": "yPykqb6Q9UNf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwaY_YcgRayy"
      },
      "source": [
        "# ðŸ”´ **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokens, seq_len):\n",
        "    # Trim tokens so that total length is divisible by seq_len\n",
        "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
        "    tokens = tokens[:n_tokens]\n",
        "\n",
        "    # Reshape to 2D tensor\n",
        "    return tokens.view(-1, seq_len)\n"
      ],
      "metadata": {
        "id": "Vp_t8qCRfNCx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "88c0c982-1ec7-4046-e834-fe0f85e3e2d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PpKbTUEIRayz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "be961f7a-72c5-494e-9097-8b76de469106"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def num_trainable_params(model):\n",
        "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_time(model, x, num_runs=10):\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(*x)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.time() - start) / num_runs\n",
        "\n",
        "\n",
        "def calculate_time_cpu(model, x, num_runs=10):\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(*x)\n",
        "    return (time.time() - start) / num_runs"
      ],
      "metadata": {
        "id": "G6LDfZvmOLcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "887545bb-41f4-449e-d9b5-cc7570bef9e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¥ tokenize Tiktoken fast"
      ],
      "metadata": {
        "id": "ocpBR9r9E0Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "RUTUzg-pGeYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e05c352-abff-4d88-c429-df0c58da7b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvHCVoih5Bap",
        "outputId": "66ba7e09-0fc3-48e3-e12a-6596a9d12f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (2119719, 1), 'validation': (21990, 1)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "tokenized_train_samples = []\n",
        "for item in tqdm(dataset[\"train\"], desc=\"Tokenizing Train Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"])\n",
        "    tokenized_train_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "id": "n9l5SC43E5zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_valid_samples = []\n",
        "for item in tqdm(dataset[\"validation\"], desc=\"Tokenizing validation Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"])\n",
        "    tokenized_valid_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "id": "LrN9LZA03mL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_valid_samples[:1]"
      ],
      "metadata": {
        "id": "cc-VMQ0251vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sumtoks=  sum(len(tok) for tok in tokenized_train_samples)\n",
        "print(sumtoks)"
      ],
      "metadata": {
        "id": "EEIO6-sDIUqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¥ Train Bpe Tokenizer and data loader"
      ],
      "metadata": {
        "id": "FfNEUe3ss4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ§ BPE Trainer"
      ],
      "metadata": {
        "id": "CMYf01JHj07X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"|<unk>|\"))\n",
        "\n",
        "# Use a pre-tokenizer to split text into words\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# Initialize a BPE trainer\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=10_000,  # Set the vocabulary size\n",
        "    special_tokens=[\"|<unk>|\", \"<|endoftext|>\"],\n",
        "    min_frequency=2,  # Set the minimum frequency of tokens\n",
        "    )\n",
        "\n",
        "# Train the tokenizer on a custom dataset\n",
        "tokenizer.train_from_iterator(dataset[\"train\"][\"text\"], trainer)\n",
        "\n",
        "# Add special tokens\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"<|endoftext|> $A\",\n",
        "    special_tokens=[(\"<|endoftext|>\", tokenizer.token_to_id(\"<|endoftext|>\"))],\n",
        ")\n",
        "\n",
        "# Add decoder\n",
        "tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# Save the trained tokenizer\n",
        "tokenizer.save(\"bpe-tokenizer_tinystories.json\")\n",
        "\n",
        "#\n",
        "print(f\"ðŸŽ‰ Tokenizer training complete!\")\n",
        "print(f\"ðŸ”¹ Vocabulary size: {tokenizer.get_vocab_size():,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0FFXwS3tf9l",
        "outputId": "8d15b6dd-aa76-4fc8-90ad-bb33aa956897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ‰ Tokenizer training complete!\n",
            "ðŸ”¹ Vocabulary size: 10,000 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
        "print(f\"ðŸŽ‰ Tokenizer training complete!\")\n",
        "print(f\"ðŸ”¹ Vocabulary size: {tokenizer.get_vocab_size():,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIB3W1p53lOc",
        "outputId": "2a50cb8d-f64b-4b18-f195-d29d7ba8f630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ‰ Tokenizer training complete!\n",
            "ðŸ”¹ Vocabulary size: 10,000 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'They played together all day and became best friends.'\n",
        "tokens = tokenizer.encode(sent)\n",
        "print(tokens.ids)\n",
        "print(tokens.tokens)\n",
        "\n",
        "pprint(tokenizer.decode(tokens.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "798_gzXZuM5x",
        "outputId": "3d103108-d7b1-4500-b626-1e9db520e12d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 546, 667, 462, 378, 252, 161, 1042, 725, 375, 15]\n",
            "['<|endoftext|>', 'They', 'Ä played', 'Ä together', 'Ä all', 'Ä day', 'Ä and', 'Ä became', 'Ä best', 'Ä friends', '.']\n",
            "'They played together all day and became best friends.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ§ Save and load Tokens with BPE tokenizer"
      ],
      "metadata": {
        "id": "H0gmGOkqIAxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization {train}\n",
        "tokenized_train_samples = []\n",
        "for item in tqdm(dataset[\"train\"], desc=\"Tokenizing Train Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"]).ids\n",
        "    tokenized_train_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "id": "NdaDekSJuq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_samples_concat=[]\n",
        "tokenized_train_samples_concat = np.concatenate(tokenized_train_samples)\n",
        "len(tokenized_train_samples_concat)"
      ],
      "metadata": {
        "id": "hi9uU-62Nq21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokens as a pytorch file\n",
        "torch.save(torch.tensor(tokenized_train_samples_concat), 'tokenized-train-samples_vocab-10k.pt')"
      ],
      "metadata": {
        "id": "rt_1F7qvQcfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization {validation}\n",
        "tokenized_valid_samples = []\n",
        "for item in tqdm(dataset[\"validation\"], desc=\"Tokenizing Validation Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"]).ids\n",
        "    tokenized_valid_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "On0JGjWju6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_valid_samples_concat=[]\n",
        "tokenized_valid_samples_concat = np.concatenate(tokenized_valid_samples)\n",
        "len(tokenized_valid_samples_concat)"
      ],
      "metadata": {
        "id": "Kz6unOJpAshB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokens as a pytorch file\n",
        "torch.save(torch.tensor(tokenized_valid_samples_concat), 'tokenized-valid-samples_vocab-10k.pt')"
      ],
      "metadata": {
        "id": "DB6a7nNC8_tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ§ Custom dataset"
      ],
      "metadata": {
        "id": "6c2dBVc4wf-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyStoriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.seq_len = seq_len\n",
        "        self.data = prepare_data(data, seq_len+1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample[:-1], sample[1:]"
      ],
      "metadata": {
        "id": "1oQvaIZLwkIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "118c7046-051b-4ed9-fae4-2fe51df0715c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = TinyStoriesDataset(tokenized_train_samples, 128)\n",
        "train_set.data.shape, len(train_set), train_set[0]"
      ],
      "metadata": {
        "id": "ZByVGGuUwenF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "bdc2a840-a69c-4d5d-cd93-71ebef3853e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-be0809126efa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyStoriesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d054a2508118>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, seq_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ff7d104d76c7>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(tokens, seq_len)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Trim tokens so that total length is divisible by seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mn_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit next(iter(train_set))"
      ],
      "metadata": {
        "id": "k47DvjFVxOdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=next(iter(train_set))"
      ],
      "metadata": {
        "id": "U0SjBcbtatUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(b)"
      ],
      "metadata": {
        "id": "6XCHXYbaaxPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ§ DataLoader"
      ],
      "metadata": {
        "id": "x2lJvLkFx-Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "ZrG3JwDrg_9e",
        "outputId": "150f6f95-1abc-4cca-e48d-e391335b5fc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/temp/bpe-tokenizer_tinystories.json\")\n",
        "tokenizer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a700lJAToJ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_train_samples = torch.load('/content/drive/MyDrive/temp/tokenized-train-samples_vocab-10k.pt')\n",
        "tokenized_valid_samples = torch.load('/content/drive/MyDrive/temp/tokenized-valid-samples_vocab-10k.pt')\n",
        "\n",
        "# train_set = TinyStoriesDataset(tokenized_train_samples, seq_len=128)\n",
        "valid_set = TinyStoriesDataset(tokenized_valid_samples, seq_len=128)"
      ],
      "metadata": {
        "id": "F5evvjhex63m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9a5fa676-17e5-4d20-8547-285fcc0389db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = DataLoader(train_set, batch_size=32, shuffle=True, pin_memory=True) #, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, pin_memory=True) #, num_workers=2)"
      ],
      "metadata": {
        "id": "0Acjf4UeyMHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e4f0d595-24c5-4e4d-b914-07cd3a5b38cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = next(iter(valid_loader))\n",
        "x_batch.shape, y_batch.shape"
      ],
      "metadata": {
        "id": "qkhNsYnMyUwe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "75e3c4ec-a4f9-42e9-d378-442ecbef505a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 128]), torch.Size([32, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_batch[0,:])\n",
        "print('\\n',y_batch[0,:])"
      ],
      "metadata": {
        "id": "YdOkfGMPnnHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(valid_loader)"
      ],
      "metadata": {
        "id": "4Em6sZ_syl2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader) / (20*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICs47WHsftyW",
        "outputId": "73583e6d-7b03-4b65-eaba-b67e11995bba"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.865"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = iter(train_loader)"
      ],
      "metadata": {
        "id": "ihV-0spZfZhq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit next(train_iter)"
      ],
      "metadata": {
        "id": "_RMcGk_qyojK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ§ EDA"
      ],
      "metadata": {
        "id": "wMtdHFsCN2Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_count_stories=[]\n",
        "for tokns in tokenized_train_samples:\n",
        "    token_count_stories.append(len(tokns))"
      ],
      "metadata": {
        "id": "UkQNex4eDn1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "106e2e8e-5632-412a-a1eb-4fce8a3b7143"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_count_stories_np=np.array(token_count_stories)"
      ],
      "metadata": {
        "id": "cxWO9YTOENR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(token_count_stories, bins=50, kde=True)\n",
        "plt.xlabel('Token Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Token Counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JRcUm7utIHkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sort(token_count_stories_np)[:1000]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LAGEzHvoN-Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¥ Transformer Model from scratch"
      ],
      "metadata": {
        "id": "vOQNBqhXMuYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtention(torch.nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv_proj = torch.nn.Linear(embed_dim, 3 * embed_dim)\n",
        "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
        "    # Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø®Ø·Ø§ Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø±Ø¯ Ø²Ù…Ø§Ù† ØªØ±ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ±ÛŒÙ† Ù†Ø´ÙˆØ¯\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        k,q,v = self.qkv_proj(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim).transpose(1,2).chunk(3)\n",
        "        # F.scaled_dot_product_attention(q,k,v)\n",
        "        # return self.out_proj(x)\n",
        "        return q"
      ],
      "metadata": {
        "id": "E7h7rW2dDss8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6e6d73a5-1530-4303-9c3a-68f49ef99d08"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.range(1,24).view(2,3,4)\n",
        "print(x)\n",
        "\n",
        "# /x=x.transpose(1,0)\n",
        "print(x)\n",
        "#"
      ],
      "metadata": {
        "id": "cZ0QUKKhQ8hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "y= MultiHeadAtention(4,2)(x)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "a0pQHFFuRGNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”´ **Model from scratch - Howsam**"
      ],
      "metadata": {
        "id": "ZnBQs-C8PNrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Define"
      ],
      "metadata": {
        "id": "oIjl-VOXpzAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "TBwuEJI6NNNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9b3f34fc-bad6-4577-fc3e-b190ce5cc6c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "VuXbNcH1p7mv",
        "outputId": "7f6e6d9d-6f29-4fef-e73f-772ec9ba8416"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Embedding"
      ],
      "metadata": {
        "id": "oP9PxAiBOXG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wte = nn.Embedding(tokenizer.get_vocab_size(), 100)\n",
        "wte(torch.tensor([1, 2,3])).shape"
      ],
      "metadata": {
        "id": "D3KuSAfszsHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "f00d94b9-1304-40c5-ae63-8c6bedc0de03"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128\n",
        "wpe = nn.Embedding(seq_len, 100)\n",
        "wpe(torch.tensor([1, 2, 100])).shape"
      ],
      "metadata": {
        "id": "NN6B1P0tzsBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "e38602bc-b3e5-438e-de99-5a6b510c327f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wpe(torch.arange(x_batch.shape[1])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "qWFIHM3x-26z",
        "outputId": "228fda44-2e3c-4f5b-cbd6-2fce909e4151"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = wte(x_batch) + wpe(torch.arange(x_batch.shape[1]))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "AMgXEhAwIJxj",
        "outputId": "dd0aa91d-9897-4f9d-e5aa-c88405578c31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 128, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "CHJ4NWXGR0YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = k = v = x\n",
        "print(q.shape)\n",
        "\n",
        "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "\n",
        "scores = q @ k.transpose(-2, -1) / (k.shape[-1]**0.5)\n",
        "scores.masked_fill_(mask ==0, float(-torch.inf))\n",
        "scores = scores.softmax(dim=-1)\n",
        "print(scores.shape)\n",
        "\n",
        "z = scores @ v\n",
        "z.shape"
      ],
      "metadata": {
        "id": "9j_qy0aIJZXl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "acaabdf9-22fe-4d4d-b0c3-3d5e0a726411"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-63054fa8b2ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = torch.randn(3, 5, 5)\n",
        "# mask = torch.tril(torch.ones(5, 5))\n",
        "# scores.masked_fill_(mask ==0, float(-torch.inf))\n",
        "# scores = scores.softmax(dim=-1)\n",
        "# scores"
      ],
      "metadata": {
        "id": "QptCjesZL96l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v):\n",
        "    mask = torch.tril(torch.ones(q.shape[-2], q.shape[-2])).to(device)\n",
        "    scores = q @ k.transpose(-2, -1) / (k.shape[-1]**0.5)\n",
        "    scores.masked_fill_(mask==0, float(-torch.inf))\n",
        "    scores = scores.softmax(dim=-1)\n",
        "    z = scores @ v\n",
        "    return z"
      ],
      "metadata": {
        "id": "mIEMC-FarKJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product_attention(x.to(device), x.to(device), x.to(device)).shape"
      ],
      "metadata": {
        "id": "wdt56H82PQqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn((128, 1024, 768), device=device)\n",
        "k = torch.randn((128, 1024, 768), device=device)\n",
        "v = torch.randn((128, 1024, 768), device=device)\n",
        "q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "xK6XgVlePa0n",
        "outputId": "fd64b71c-d265-445d-b77a-5abd363a73d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1024, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product_attention(q, k, v).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5COTL0OKPxY1",
        "outputId": "46cc32a9-f255-4508-9fc3-9f1cbf376790"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scaled_dot_product_attention' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d35194045c40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'scaled_dot_product_attention' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate_time(scaled_dot_product_attention, (q, k, v), num_runs=20)\n",
        "calculate_time_cpu(scaled_dot_product_attention, (q, k, v), num_runs=20)"
      ],
      "metadata": {
        "id": "LsaSSCkaQLtw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.scaled_dot_product_attention(q, k, v, is_causal=True).shape"
      ],
      "metadata": {
        "id": "UmCJ2BV6Unge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(scaled_dot_product_attention(q, k, v) - F.scaled_dot_product_attention(q, k, v, is_causal=True)).max()"
      ],
      "metadata": {
        "id": "IjrAqzTWU79x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(F.scaled_dot_product_attention, (q, k, v), num_runs=20)"
      ],
      "metadata": {
        "id": "cnGXNViJVWzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Multi Head Attention"
      ],
      "metadata": {
        "id": "rUjFcYeiIE9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MultiHeadAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(100, 1000)\n",
        "#         self.fc2 = nn.Linear(1000, 100)\n",
        "#         self.fc3 = nn.Linear(1000, 100)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         y = F.relu(self.fc1(x))\n",
        "#         y1 = self.fc2(y)\n",
        "#         y2 = self.fc3(y)\n",
        "#         return F.relu(torch.concat([y1, y2], dim=-1))"
      ],
      "metadata": {
        "id": "gnxYy1oIQcY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "601d8702-3d97-430e-affc-d934c8994ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mha = MultiHeadAttention()\n",
        "# num_trainable_params(mha)\n",
        "# mha.forward(torch.rand(10, 100)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Et6wVx6f7HmB",
        "outputId": "3271b803-66f5-40db-80b8-de54a91a19df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x=torch.randn(2,4)\n",
        "# print(x)\n",
        "# lx=nn.Linear(4,8,bias=False)\n",
        "# y1 =x@ lx.weight.T\n",
        "# y2=lx(x)\n",
        "# print(y1.softmax(dim=-1).argmax(dim=0))\n",
        "# print(y1.softmax(dim=-1))\n",
        "# print(lx.weight.T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7FtEClDCBhKQ",
        "outputId": "b869524b-2324-415f-8448-eeb4dbad62cf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    n_embd: int = 100\n",
        "    n_head: int = 5\n",
        "\n",
        "config = GPTConfig()\n",
        "config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "q10jLOLsuzxi",
        "outputId": "bddc5cde-3f52-442d-d0ae-9c7326dbc2c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.head_size = self.n_embd // self.n_head\n",
        "\n",
        "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
        "\n",
        "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.c_proj.residual = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y #,q, k, v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2X9b79Iq8l38",
        "outputId": "c26667b0-ccc4-4568-b722-3dfb19fd62f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mha = MultiHeadAttention(config)\n",
        "# y,q, k, v= mha(x)\n",
        "# print(\"X:\",x.shape)\n",
        "# print(\"qkv_proj:\",mha.qkv_proj.weight.T.shape)\n",
        "# print(\"c_proj:\",mha.c_proj.weight.T.shape)\n",
        "\n",
        "# print(\"q:\",q.shape)\n",
        "# print(k.shape)\n",
        "# print(v.shape)\n",
        "# print(\"y:\",y.shape)"
      ],
      "metadata": {
        "id": "vA3Nw2Riv1Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(mha.to(device), (x.to(device),), num_runs=20)"
      ],
      "metadata": {
        "id": "3YRyw8Hu-bXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Feed Forward (MLP)"
      ],
      "metadata": {
        "id": "WTVewgG-m7nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    n_embd: int = 100\n",
        "    n_head: int = 5\n",
        "    f_expnd: float = 4\n",
        "\n",
        "config = GPTConfig()\n",
        "config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "R-r7XatDAP4B",
        "outputId": "824fd999-ea8d-4728-f0c4-cdb387039d14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.f_expnd = config.f_expnd\n",
        "\n",
        "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
        "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
        "        self.down_proj.residual = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(F.gelu(self.up_proj(x)))"
      ],
      "metadata": {
        "id": "BTcx4J5Lm66z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "09a70790-ae3e-42d4-edd0-41b31f7e5712"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x1=torch.randn(2,4,100)\n",
        "# plt.hist(x1.flatten(), bins=50);\n",
        "# plt.show()\n",
        "# plt.hist(F.gelu(x1).flatten(), bins=50);\n",
        "# plt.show()\n",
        "# F.gelu(x1).min(dim=-1)"
      ],
      "metadata": {
        "id": "aJUGrkOkm6tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feedfor = FeedForward(config)\n",
        "print(feedfor(x).shape)\n",
        "feedfor.up_proj.weight.T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "aDU2oB-PAfXX",
        "outputId": "41400bfa-5b7f-408d-c919-db9af8cc8e35"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-95be6d2b032e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeedfor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedfor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeedfor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params(feedfor)*1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "JtLY269MBHD3",
        "outputId": "37d39567-148e-4711-c783-0b2fa4dcc63c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(mlp, (x, ), num_runs=20)"
      ],
      "metadata": {
        "id": "FKYxPvrzBQbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Decoder Block"
      ],
      "metadata": {
        "id": "ih6sV9ljndzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.mha = MultiHeadAttention(config)\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gq1kw31av9DR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a9fd3ac9-ec60-4e74-de75-8dbb454660cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = DecoderBlock(config)\n",
        "y = decoder(x)\n",
        "print(x.shape)\n",
        "plt.hist(x.detach().flatten(), bins=50);\n",
        "plt.show()\n",
        "plt.hist(y.detach().flatten(), bins=50);\n",
        "plt.show()\n",
        "\n",
        "means_before = x.mean(dim=-1)  # shape: (32, 128)\n",
        "means_after = y.mean(dim=-1)\n",
        "\n",
        "stds_before = x.std(dim=-1)\n",
        "stds_after = y.std(dim=-1)\n",
        "\n",
        "plt.hist(means_before.detach().flatten(), bins=50);\n",
        "plt.show()\n",
        "plt.hist(means_after.detach().flatten(), bins=50);\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "AweCYj03EtPy",
        "outputId": "2ae361de-3662-4a22-f409-9fa1104a7e32"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-061b9b612b08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norml=nn.LayerNorm(100)\n",
        "xrnd=torch.range(1,100)\n",
        "print(xrnd.min().item(), xrnd.max().item(), xrnd.mean().item(), xrnd.std().item())\n",
        "print(norml(xrnd).min().item(), norml(xrnd).max().item(),norml(xrnd).mean().item(),9 , norml(xrnd).std().item())\n",
        "plt.hist(xrnd, bins=10);\n",
        "plt.show()\n",
        "plt.hist(norml(xrnd).detach(), bins=10);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TJdKmDWRyf2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params(decoder) * 1e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "pO5KeXeLFmDk",
        "outputId": "23b8ff5e-e9d1-4c6c-efff-a8b3449dfb28"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'decoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-202477f16d8e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_trainable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time_cpu(decoder, (x, ), num_runs=20) * 1e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "7zAYllgcFsfg",
        "outputId": "88afe093-aaf2-4a2a-9702-9a8ed1a2f320"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.37346315383911"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  GPT"
      ],
      "metadata": {
        "id": "LCzTHf8iitEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    vocab_size: int = 10_000\n",
        "    seq_len: int = 128\n",
        "    n_layer: int = 12\n",
        "    n_embd: int = 100\n",
        "    n_head: int = 5\n",
        "    f_expnd: float = 4\n",
        "\n",
        "\n",
        "config = GPTConfig()\n",
        "config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "adRz9KC8HbbT",
        "outputId": "a9141e40-f01f-4c39-d8db-3be4538bdd53"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.seq_len, config.n_embd)\n",
        "        # self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "        self.lnf = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.lm_head.weight = self.wte.weight\n",
        "        # self.lm_head.weight.data.uniform_(-1/self.lm_head.in_features**0.5, 1/self.lm_head.in_features**0.5)\n",
        "        # nn.init.uniform_(self.lm_head.weight, -1/self.lm_head.in_features**0.5, 1/self.lm_head.in_features**0.5)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'residual'):\n",
        "                std *= (2*self.config.n_layer)**-0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
        "\n",
        "        # x = self.decoders(x)\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x)\n",
        "\n",
        "        x = self.lnf(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Lmrc034JwvSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "240f1812-4400-40d2-8d60-fdb72e6975c2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(config).to(device)\n",
        "logits= model(x_batch.to(device))\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "FwYwWZ_Z-dV-",
        "outputId": "8bc0928d-0875-469b-8d9a-0819c4586dd5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FeedForward' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e4d096bf88f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-ba613d993dcb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-ba613d993dcb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-1cf5dd95b63b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FeedForward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦¾ ØªÙ…Ø±ÛŒÙ†:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Ø¨Ø®Ø´ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø±Ø§ Ø¬Ù†Ø±ÛŒØª Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¬ÛŒ Ù¾ÛŒ ØªÛŒ Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ø±Ú¯ Ù…Ú©Ø³ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯"
      ],
      "metadata": {
        "id": "Tf7SiFsgp99a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_tokens= torch.argmax(F.softmax(logits,dim=-1), dim=-1)\n",
        "pprint(tokenizer.decode(x_batch[0,:].tolist()))\n",
        "print('\\n\\n')\n",
        "pprint(tokenizer.decode(gen_tokens[0,:].tolist()))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "BEcruFtckoln",
        "outputId": "b9158436-bf2e-4340-e253-127f2c530a96"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'logits' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-67f18321f891>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_tokens\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "novLGFNRkUYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.lm_head.weight.T.shape , model.wte.weight.T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "CWsuN52NNtYV",
        "outputId": "700b9330-4634-49c7-cc2b-7029aeeaa15d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d0ae9be3fc86>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params(model), num_trainable_params(model.decoders), num_trainable_params(model.lm_head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "JdecDmI3-rvN",
        "outputId": "a7573aa5-f30d-4169-f4f1-331044894992"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.4578, 1.4448, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(model, (x_batch.to(device),), num_runs=100) * 1e3"
      ],
      "metadata": {
        "id": "gxIo0WS7-z9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ  Initialization"
      ],
      "metadata": {
        "id": "KEnoziDiCZ84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(\n",
        "    GPTConfig).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UVGTRYO9FH3O",
        "outputId": "c0fcbc55-40f0-4ec9-972e-22757ea4cb3d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model.decoders[0].mha.c_proj.weight.flatten().detach().cpu(), bins=50);"
      ],
      "metadata": {
        "id": "dy1ErMbaCeNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.02 * (2*4)**-0.5 * 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "OnIIvdoUIjjO",
        "outputId": "d22bef45-2688-4ac2-f231-2d9ad77406e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021213203435596427"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model.wpe.weight.flatten()[:100_000].detach().cpu(), bins=50);"
      ],
      "metadata": {
        "id": "l10kLsGcFzAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model)"
      ],
      "metadata": {
        "id": "ZAbJNo57IyP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model.decoders[2].mlp.down_proj.weight.flatten().detach().cpu(), bins=50);"
      ],
      "metadata": {
        "id": "DB-vyDHpI28x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¥ GPT model implement with nn.torch transformer"
      ],
      "metadata": {
        "id": "uVkN_q8j7Dpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦¾ ØªÙ…Ø±ÛŒÙ†\n",
        "\n",
        "\n",
        "Ø´Ø¨Ú©Ù‡ Ø¬ÛŒ Ù¾ÛŒ ØªÛŒ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ù¾Ø§ÛŒ ØªÙˆØ±Ú† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ø§ Ù…Ø¯Ù„ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø§Ø² ØµÙØ± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯."
      ],
      "metadata": {
        "id": "y0lYZZAw5rw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = torch.nn.Transformer(d_model=100, nhead=5, num_encoder_layers=0, num_decoder_layers=12, dim_feedforward=400,\n",
        "                     dropout=0.1, activation=\"gelu\", custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05,\n",
        "                     batch_first=False, norm_first=False, bias=True, device=None, dtype=None)"
      ],
      "metadata": {
        "id": "8lp68U6i7gkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "f351958b-0702-49a1-a618-fe8e6ba55adc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_layer = nn.TransformerDecoderLayer(d_model= 100, nhead=5, dim_feedforward=400)\n",
        "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=12)\n",
        "memory = torch.rand(10, 32, 100)\n",
        "tgt = torch.rand(20, 32, 100)\n",
        "out = transformer_decoder(tgt, memory)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "nzcZJ2jss9e2",
        "outputId": "59f119a7-4620-47aa-f948-3328f037f116"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTTorch(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.seq_len, config.n_embd)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model= config.n_embd, nhead=config.n_head,\n",
        "                                                   dim_feedforward=config.f_expnd*config.n_embd,activation=F.gelu,norm_first=True\n",
        "                                                   )\n",
        "        # decoder_layer.self_attn.is_causal = True\n",
        "        self.decoders = nn.TransformerDecoder(decoder_layer, num_layers=12)\n",
        "        # self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "        self.lnf = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.lm_head.weight = self.wte.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'residual'):\n",
        "                std *= (2*self.config.n_layer)**-0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
        "\n",
        "        # x = self.decoders(x)\n",
        "        # for decoder in self.decoders:\n",
        "        x = self.decoders(x ,memory=None)\n",
        "\n",
        "        x = self.lnf(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "58fa192f-25d7-47fb-a820-b543d3374d3a",
        "id": "6C0k8lUSwAKT"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTTorch(config).to(device)\n",
        "logits= model(x_batch.to(device))\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "2_2QuMoa1KXV",
        "outputId": "58549a8f-7c7d-4ce2-ee2f-d0338ef88b95"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'is_nested'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-3993b6555f2c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-6ef006ed7e03>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# x = self.decoders(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# for decoder in self.decoders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    614\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             )\n\u001b[0;32m-> 1098\u001b[0;31m             x = x + self._mha_block(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mis_causal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     ) -> Tensor:\n\u001b[0;32m-> 1148\u001b[0;31m         x = self.multihead_attn(\n\u001b[0m\u001b[1;32m   1149\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                     )\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0many_nested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m         assert not any_nested, (\n\u001b[1;32m   1331\u001b[0m             \u001b[0;34m\"MultiheadAttention does not support NestedTensor outside of its fast path. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_nested'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_tokens= torch.argmax(F.softmax(logits,dim=-1), dim=-1)\n",
        "pprint(tokenizer.decode(x_batch[0,:].tolist()))\n",
        "print('\\n\\n')\n",
        "pprint(tokenizer.decode(gen_tokens[0,:].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "wRd4kbT91YSR",
        "outputId": "f2c681a0-5fce-40e5-f1d3-9361c81ba7fa"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright '\n",
            " 'and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every '\n",
            " 'day.\"\\n'\n",
            " '\\n'\n",
            " 'After playing with the car, Kitty and Spot felt thirsty. They found a small '\n",
            " 'pond with clear water. They drank the water and felt very happy. They played '\n",
            " 'together all day and became best friends.Once upon a time, in a big forest, '\n",
            " 'there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, '\n",
            " 'rocks, and hills. One day, Roxy found an icy hill. She')\n",
            "\n",
            "\n",
            "\n",
            "(' throws singer becoming disgust mouse disgust disgust His earn farm ideas '\n",
            " 'disgustothyextext farmendant farm farm farm farmendantendantendant opera smo '\n",
            " 'farm glided intelligink donÃ¢iments intellig smo smoimentsÅ“WhereÅ“Where '\n",
            " 'plantroom att bru rang rang donÃ¢ donÃ¢ att successful promisedext meowing '\n",
            " 'slower as blood h att blood hhen cuts h htedink hhenhenMaya exc beetle att '\n",
            " 'slower gap h h Carl asounced hawkhenhenlephantext aston wriggled blood h '\n",
            " 'chairs paintinghen comple activitiesext hours painting pistol mittens '\n",
            " 'hextlephant blood emergencyext argumBob dragext Avaexthog line '\n",
            " 'hayMaddieTruelinghenextï¿½ tant miracle h natural pout teach teach charm teach '\n",
            " 'teach')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦¾ ØªÙ…Ø±ÛŒÙ†\n",
        "\n",
        "Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ§ÛŒÙ†ÛŒ Ø§Ø³ØªÙˆØ±ÛŒ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø±Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø³Ø§Ø²ÛŒØ¯.\n",
        "\n",
        "Howsam |vocab_size:10_000| seq_len:128| n_layer:12| n_embd:100|n_head:5| f_expnd: 4  \n",
        "\n",
        "GPT-14 |vocab_size:50304| seq_len:1024| n_layer:12| n_embd:768|n_head:12| f_expnd: 4| dropout: 0.0| bias:True\n",
        "\n",
        "gpt_neo | vocab_size:50257| seq_len:256| n_layer:8| n_embd:256|n_head:16| f_expnd: --| dropout: 0.1| torch_dtype:float16\n"
      ],
      "metadata": {
        "id": "EThI0ZACnJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦¾  ØªÙ…Ø±ÛŒÙ†\n",
        "\n",
        "\n",
        "Ø§Ù„Ø¨ØªÙ‡! Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø§Ø² **Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ** Ø³Ù‡ Ù…Ø¯Ù„ Ù…Ø¹Ø±ÙˆÙ Ø§Ø² Ø¯Ùˆ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ù…Ø®ØªÙ„Ù Ø¢ÙˆØ±Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª:\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Encoder-only (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ BERT)\n",
        "\n",
        "### ðŸ“Œ 1. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "\n",
        "* **Ù†ÙˆØ¹**: Encoder-only\n",
        "* **Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ**: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚ Ø²Ø¨Ø§Ù†ÛŒ Ø§Ø² Ù…ØªÙ† Ø¨Ø§ Ø®ÙˆØ§Ù†Ø¯Ù† **Ø¯Ùˆ Ø·Ø±ÙÙ‡** (bidirectional).\n",
        "* **Ø±ÙˆØ´ Ø¢Ù…ÙˆØ²Ø´**:\n",
        "\n",
        "  * **Masked Language Modeling (MLM)**: Ø¨Ø¹Ø¶ÛŒ Ø§Ø² ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ù…Ø§Ø³Ú© Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ùˆ Ù…Ø¯Ù„ Ø¨Ø§ÛŒØ¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯.\n",
        "  * **Next Sentence Prediction (NSP)**: Ø¢ÛŒØ§ Ø¬Ù…Ù„Ù‡ Ø¯ÙˆÙ… Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡ØŸ\n",
        "* **Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§**: Ø¯Ø±Ú© Ø²Ø¨Ø§Ù†ØŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ†ØŒ Named Entity Recognition Ùˆ ...\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ 2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**\n",
        "\n",
        "* **Ù†ÙˆØ¹**: Encoder-only (Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± BERT)\n",
        "* **Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ**: Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª Ø¢Ù…ÙˆØ²Ø´ BERT Ø¨Ø§ Ø­Ø°Ù NSPØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¨ÛŒØ´ØªØ± Ùˆ batchÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯â€ŒØªØ±.\n",
        "* **ØªÙØ§ÙˆØª Ø¨Ø§ BERT**:\n",
        "\n",
        "  * ÙÙ‚Ø· Ø§Ø² MLM Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "  * Ø§Ø² shuffling Ø¨Ù‡ØªØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ dynamic masking Ø¨Ù‡Ø±Ù‡ Ù…ÛŒâ€ŒØ¨Ø±Ø¯.\n",
        "* **Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§**: Ù…Ø´Ø§Ø¨Ù‡ BERT Ø§Ù…Ø§ Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± Ø¯Ø± Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² ØªØ³Øªâ€ŒÙ‡Ø§.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ 3. **ELECTRA**\n",
        "\n",
        "* **Ù†ÙˆØ¹**: Encoder-only (ÙˆÙ„ÛŒ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… Ù…ØªÙØ§ÙˆØª)\n",
        "* **Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ**: Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ø³Ú©â€ŒØ´Ø¯Ù‡ (Ù…Ø§Ù†Ù†Ø¯ BERT)ØŒ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ú©Ù‡ **Ø¢ÛŒØ§ ÛŒÚ© ØªÙˆÚ©Ù† ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª ÛŒØ§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù‡**.\n",
        "* **Ø±ÙˆØ´ Ø¢Ù…ÙˆØ²Ø´**:\n",
        "\n",
        "  * ÛŒÚ© generator ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "  * ÛŒÚ© discriminator ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ú©Ø¯Ø§Ù… ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n",
        "* **Ù…Ø²ÛŒØª**: Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ Ú©Ø§Ø±Ø¢Ù…Ø¯ØªØ± Ø§Ø² BERT Ø§Ø² Ù†Ø¸Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Decoder-only (Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ GPT)\n",
        "\n",
        "### ðŸ“Œ 1. **GPT-2**\n",
        "\n",
        "* **Ù†ÙˆØ¹**: Decoder-only\n",
        "* **Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ**: Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø²Ø¨Ø§Ù† Ø¨Ù‡â€ŒØµÙˆØ±Øª **Ú†Ù¾ Ø¨Ù‡ Ø±Ø§Ø³Øª** (causal), Ø¨Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙˆÚ©Ù† Ø¨Ø¹Ø¯ÛŒ.\n",
        "* **Ø±ÙˆØ´ Ø¢Ù…ÙˆØ²Ø´**:\n",
        "\n",
        "  * ÙÙ‚Ø· Ø§Ø² **Language Modeling (Auto-regressive)** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "* **Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§**: ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒØŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ù¾Ø±Ø³Ø´ØŒ ØªØ±Ø¬Ù…Ù‡ Ùˆ ...\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ 2. **GPT-3**\n",
        "\n",
        "* **Ù†ÙˆØ¹**: Decoder-only (ØªÙˆØ³Ø¹Ù‡â€ŒÛŒØ§ÙØªÙ‡â€ŒØªØ± Ø§Ø² GPT-2)\n",
        "* **Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ**: Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ø´Ø¯ÛŒØ¯ (175B Ù¾Ø§Ø±Ø§Ù…ØªØ±) + ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø§Ù†Ø¬Ø§Ù… Ú†Ù†Ø¯ÙˆØ¸ÛŒÙÙ‡â€ŒØ§ÛŒ Ø¨Ø¯ÙˆÙ† fine-tuning.\n",
        "* **ÙˆÛŒÚ˜Ú¯ÛŒ Ø®Ø§Øµ**: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù†Ù…ÙˆÙ†Ù‡ (Few-shot / Zero-shot Learning).\n",
        "* **Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§**: Ú†Øªâ€ŒØ¨Ø§ØªØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ØŒ Ù…Ù‚Ø§Ù„Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ùˆ...\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ 3. **LLaMA (Large Language Model Meta AI)**\n",
        "\n",
        "* **Ù†ÙˆØ¹**: Decoder-only\n",
        "* **Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ**: Ø·Ø±Ø§Ø­ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø¨Ú©â€ŒØªØ± Ø§Ù…Ø§ Ø¨Ø³ÛŒØ§Ø± Ú©Ø§Ø±Ø¢Ù…Ø¯ (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ú©Ù…ØªØ± Ø§Ø² GPT-3 Ø§Ù…Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø´Ø§Ø¨Ù‡).\n",
        "* **Ù…Ø²ÛŒØª**: Ø¨Ø§Ø² Ø¨ÙˆØ¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ù…Ø­Ø¯ÙˆØ¯.\n",
        "* **Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§**: Ú†Øªâ€ŒØ¨Ø§ØªØŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒØŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø¨Ø§Ø².\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ø¯ÙˆÙ„â€ŒÙˆØ§Ø±:\n",
        "\n",
        "| Ù…Ø¯Ù„     | Ù†ÙˆØ¹ Ù…Ø¹Ù…Ø§Ø±ÛŒ   | Ø±ÙˆØ´ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§ØµÙ„ÛŒ           | Ú©Ø§Ø±Ø¨Ø±Ø¯ Ú©Ù„ÛŒØ¯ÛŒ                |\n",
        "| ------- | ------------ | -------------------------- | --------------------------- |\n",
        "| BERT    | Encoder-only | MLM + NSP                  | Ø¯Ø±Ú© Ø²Ø¨Ø§Ù†                    |\n",
        "| RoBERTa | Encoder-only | MLM (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡)            | Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Ø¯Ø±Ú© Ù¾ÛŒØ´Ø±ÙØªÙ‡     |\n",
        "| ELECTRA | Encoder-only | Discriminator-Generator    | Ø¢Ù…ÙˆØ²Ø´ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±     |\n",
        "| GPT-2   | Decoder-only | Auto-regressive LM         | ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†                   |\n",
        "| GPT-3   | Decoder-only | Few-shot + Auto-regressive | Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ú†Ù†Ø¯Ú©Ø§Ø±Ù‡            |\n",
        "| LLaMA   | Decoder-only | Efficient auto-regressive  | ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø³Ø¨Ú©â€ŒØªØ± |\n",
        "\n",
        "---\n",
        "\n",
        "Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Encoder-Decoder Ù…Ø«Ù„ T5 ÛŒØ§ BART Ø±Ùˆ Ù‡Ù… Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…ØŒ ÙÙ‚Ø· Ø¨Ú¯Ùˆ.\n"
      ],
      "metadata": {
        "id": "3efgNYrZ3u77"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86bxO68Z32Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦¾ ØªÙ…Ø±ÛŒÙ†\n",
        "\n",
        "Ø¨Ø§ Ú†Ù‡ ØªØºÛŒÛŒØ±Ø§ØªÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† GPT Ø±Ø§ Ø¨Ù‡ Ù…Ø¯Ù„ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Llama 3.2 ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯ØŸ Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯.\n",
        "\n",
        "Ø§ÛŒÙ† Ú©Ø¯ ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… **Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„ LLaMA 3** Ø§Ø² Ø´Ø±Ú©Øª **Meta AI** Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§Ø² Ù†ÙˆØ¹ **Decoder-only** Ùˆ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ Transformer Ø§Ø³ØªØŒ Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ **Rotary Embeddings**ØŒ **Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ (KV Caching)**ØŒ Ùˆ **Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØ§Ø²ÛŒ (Model Parallelism)** Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ `fairscale`.\n",
        "\n",
        "Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø¢Ù† Ø±Ø§ Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù… Ùˆ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒÙ…:\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ú©Ø¯\n",
        "\n",
        "| Ø¨Ø®Ø´                                                         | ØªÙˆØ¶ÛŒØ­                                                        |\n",
        "| ----------------------------------------------------------- | ------------------------------------------------------------ |\n",
        "| `RMSNorm`                                                   | Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ RMSØŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø³Ø¨Ú©â€ŒØªØ± Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ø¨Ø±Ø§ÛŒ LayerNorm    |\n",
        "| `apply_scaling`, `precompute_freqs_cis`, `apply_rotary_emb` | Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÛŒØ§Ø¨ÛŒ Ú†Ø±Ø®Ø´ÛŒ (RoPE)                          |\n",
        "| `Attention`                                                 | Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Attention Ø¨Ø§ Ú©Ø´ (KV cache) Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² GQA |\n",
        "| `FeedForward`                                               | Ø´Ø¨Ú©Ù‡ FFN Ø¯Ùˆ Ø´Ø§Ø®Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ ØºÛŒØ±Ø®Ø·ÛŒ (SiLU)                 |\n",
        "| `TransformerBlock`                                          | ÛŒÚ© Ø¨Ù„ÙˆÚ© Ú©Ø§Ù…Ù„ Ø´Ø§Ù…Ù„ Attention + FFN                            |\n",
        "| `Transformer`                                               | Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ LLaMA Ø´Ø§Ù…Ù„ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯ØŒ embedding Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ  |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ù‡Ù… Ù‡Ø± Ø¨Ø®Ø´\n",
        "\n",
        "### ðŸ§  1. `RMSNorm`\n",
        "\n",
        "```python\n",
        "class RMSNorm(nn.Module):\n",
        "```\n",
        "\n",
        "Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Â«Ø±ÛŒØ´Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø±Ø¨Ø¹Ø§ØªÂ»ØŒ Ø¨Ø¯ÙˆÙ† Ú©Ù…â€ŒÚ©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†. Ø§ÛŒÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ø§Ø³Øª Ù†Ø³Ø¨Øª Ø¨Ù‡ LayerNorm.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŒ€ 2. `Rotary Embeddings`\n",
        "\n",
        "```python\n",
        "def apply_rotary_emb(xq, xk, freqs_cis): ...\n",
        "```\n",
        "\n",
        "Ø§ÛŒØ¯Ù‡ RoPE Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª ØªÙˆØ§Ø¨Ø¹ Ø³ÛŒÙ†ÙˆØ³ÛŒ Ù¾ÛŒÚ†Ø´ÛŒ ÙˆØ§Ø±Ø¯ ÙØ¶Ø§ÛŒ attention Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ±ØªÛŒØ¨ÛŒ (position) Ø¯Ø± dot-product attention Ù„Ø­Ø§Ø¸ Ø´ÙˆØ¯.\n",
        "\n",
        "* `precompute_freqs_cis` ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ Ù¾ÛŒØ´â€ŒÙ…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "* `apply_rotary_emb` Ø§ÛŒÙ† ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ query Ùˆ key Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "\n",
        "---\n",
        "\n",
        "### âš¡ 3. `Attention`\n",
        "\n",
        "```python\n",
        "class Attention(nn.Module): ...\n",
        "```\n",
        "\n",
        "Ù„Ø§ÛŒÙ‡ attention Ù…Ø¯Ù„ Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ±:\n",
        "\n",
        "* **GQA**: Grouped Query Attention (Ù…Ø«Ù„ LLaMA 2/3).\n",
        "* **Model Parallelism**: Ø¨Ø§ `ColumnParallelLinear` Ùˆ `RowParallelLinear`.\n",
        "* **KV Cache**: Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø´ØªÙ† key/value Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¯Ø± Ø·ÙˆÙ„ decoding.\n",
        "* **Rotary Embeddings**: Ø¨Ø§ `apply_rotary_emb` Ø±ÙˆÛŒ xq/xk.\n",
        "* **Causal Masking**: Ø¨Ø§ `mask` Ø¯Ø± dot-product attention Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù†Ú¯Ø§Ù‡ Ø¨Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡.\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ 4. `FeedForward`\n",
        "\n",
        "```python\n",
        "class FeedForward(nn.Module): ...\n",
        "```\n",
        "\n",
        "ÛŒÚ© Ù„Ø§ÛŒÙ‡ FFN Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ hidden dimension Ú¯Ø³ØªØ±Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Ø¨Ø§ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ SiLU Ùˆ Ø¶Ø±Ø¨ pointwise Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ø² Û³ Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ (w1ØŒ w2ØŒ w3) Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§± 5. `TransformerBlock`\n",
        "\n",
        "```python\n",
        "class TransformerBlock(nn.Module): ...\n",
        "```\n",
        "\n",
        "ÛŒÚ© Ø¨Ù„Ø§Ú© Ú©Ø§Ù…Ù„ Ø´Ø§Ù…Ù„:\n",
        "\n",
        "* `RMSNorm â†’ Attention â†’ Residual`\n",
        "* `RMSNorm â†’ FFN â†’ Residual`\n",
        "\n",
        "Ø¨Ø§ ØªØ±ØªÛŒØ¨ Ù…Ø´Ø§Ø¨Ù‡ Ù…Ø¹Ù…Ø§Ø±ÛŒ LLaMA (norm-first).\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ—ï¸ 6. `Transformer`\n",
        "\n",
        "```python\n",
        "class Transformer(nn.Module): ...\n",
        "```\n",
        "\n",
        "Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø´Ø§Ù…Ù„:\n",
        "\n",
        "* Embedding Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ `VocabParallelEmbedding`\n",
        "* Ù¾Ø´ØªÙ‡â€ŒØ§ÛŒ Ø§Ø² `TransformerBlock`s\n",
        "* Normalization Ù†Ù‡Ø§ÛŒÛŒ\n",
        "* Ø®Ø±ÙˆØ¬ÛŒ `ColumnParallelLinear` Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "\n",
        "Ù‡Ù…Ú†Ù†ÛŒÙ†:\n",
        "\n",
        "* `precompute_freqs_cis`: ÙØ±Ú©Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ RoPE Ø§Ø² Ù‚Ø¨Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "* `@torch.inference_mode()`: Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø§ÛŒÙ† `forward()` Ø¨Ø±Ø§ÛŒ inference Ø§Ø³Øª.\n",
        "* `mask`: Ø¨Ø±Ø§ÛŒ causal attention Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2qPUKSqaAux4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦¾ ØªÙ…Ø±ÛŒÙ†\n",
        "\n",
        "Ù…Ù‚Ø§Ù„Ø§Øª TransformerØŒ BERTØŒ GPT-2 Ùˆ GPT-3 Ø±Ø§ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯. Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø®Ø· Ø¨Ù‡ Ø®Ø· Ù†ÛŒØ³ØªØ› Ù…Ø±ÙˆØ± Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ú©Ø§ÙÛŒ Ø§Ø³Øª. ØŸ\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· 1. **Transformer (Vaswani et al., 2017) â€“ \"Attention is All You Need\"**\n",
        "\n",
        "### ðŸ§  Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ:\n",
        "\n",
        "> Ú©Ù†Ø§Ø± Ú¯Ø°Ø§Ø´ØªÙ† Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ù…Ø«Ù„ RNN Ùˆ LSTM Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ ØµØ±Ù Ø§Ø² **Ù…Ú©Ø§Ù†ÛŒØ²Ù… Attention** Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ (sequences).\n",
        "\n",
        "### ðŸ“Œ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:\n",
        "\n",
        "* **Self-Attention:** Ù‡Ø± ØªÙˆÚ©Ù† Ø¨Ø§ Ø³Ø§ÛŒØ± ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ ØªØ¹Ø§Ù…Ù„ Ø¯Ø§Ø±Ø¯ ØªØ§ Ø²Ù…ÛŒÙ†Ù‡Ù” Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ù‡ØªØ± Ø¯Ø±Ú© Ú©Ù†Ø¯.\n",
        "* **Multi-Head Attention:** Ú†Ù†Ø¯ÛŒÙ† attention Ù…ÙˆØ§Ø²ÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù Ø±ÙˆØ§Ø¨Ø· Ø±Ø§ Ø¨ÛŒØ§Ù…ÙˆØ²Ø¯.\n",
        "* **Ø¨Ø¯ÙˆÙ† RNN ÛŒØ§ CNN**: Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…ÙˆØ§Ø²ÛŒâ€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ØŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´.\n",
        "* Ù…Ø¯Ù„ Ø´Ø§Ù…Ù„ ÛŒÚ© **encoder-decoder** Ø¨Ø§ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ attention Ùˆ feed-forward Ø§Ø³Øª.\n",
        "\n",
        "### ðŸŒ ØªØ£Ø«ÛŒØ±:\n",
        "\n",
        "Ù¾Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø± ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø¹Ø¯ÛŒ Ù…Ø«Ù„ BERTØŒ GPTØŒ T5 Ùˆ LLaMA.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· 2. **BERT (Devlin et al., 2018) â€“ \"Bidirectional Encoder Representations from Transformers\"**\n",
        "\n",
        "### ðŸ§  Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ:\n",
        "\n",
        "> Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ **Ø¯ÙˆØ·Ø±ÙÙ‡ (bidirectional)** Ú©Ù‡ Ú©Ù„ Ø¬Ù…Ù„Ù‡ Ø±Ø§ (Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯) Ø¯Ø±Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯Ø› Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø§Ø²â€ŒÙ¾ÛŒØ´ (pretraining) Ùˆ Ø¨Ø¹Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ÙˆØ¸Ø§ÛŒÙ Ù…Ø®ØªÙ„Ù.\n",
        "\n",
        "### ðŸ“Œ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:\n",
        "\n",
        "* Ø§Ø² **encoder**Ù‡Ø§ÛŒ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ù†Ù‡ decoder).\n",
        "* **Masked Language Modeling (MLM):** Ø¯Ø± Ø­ÛŒÙ† Ø¢Ù…ÙˆØ²Ø´ØŒ Ø¨Ø±Ø®ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ù¾Ù†Ù‡Ø§Ù† Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù…Ø¯Ù„ Ø¨Ø§ÛŒØ¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø­Ø¯Ø³ Ø¨Ø²Ù†Ø¯.\n",
        "* **Next Sentence Prediction (NSP):** ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ù¾Ø´Øªâ€ŒØ³Ø±Ù‡Ù… Ø¨Ù‡ Ù‡Ù… Ù…Ø±Ø¨ÙˆØ·â€ŒØ§Ù†Ø¯ ÛŒØ§ Ù†Ù‡.\n",
        "* Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ **Ø¯Ø±Ú© Ù…ØªÙ†**ØŒ Ù†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†.\n",
        "\n",
        "### ðŸŒ ØªØ£Ø«ÛŒØ±:\n",
        "\n",
        "Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ Ø¯Ø± **Ø¯Ø±Ú© Ø²Ø¨Ø§Ù†** (Ù…Ø«Ù„Ø§Ù‹ Ù¾Ø§Ø³Ø®â€Œ Ø¨Ù‡ Ø³ÙˆØ§Ù„ØŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ ØªØ±Ø¬Ù…Ù‡). Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ RoBERTaØŒ ALBERTØŒ Ùˆ DistilBERT.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· 3. **GPT-2 (Radford et al., 2019) â€“ \"Language Models are Unsupervised Multitask Learners\"**\n",
        "\n",
        "### ðŸ§  Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ:\n",
        "\n",
        "> ÛŒÚ© Ù…Ø¯Ù„ **Ø²Ø¨Ø§Ù† Ø®ÙˆØ¯Ø±Ú¯Ø±Ø³ÛŒÙˆ (Auto-regressive)** Ú©Ù‡ ÙÙ‚Ø· Ø¨Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø·ÛŒÙ ÙˆØ³ÛŒØ¹ÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ø²Ø¨Ø§Ù†ÛŒ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯.\n",
        "\n",
        "### ðŸ“Œ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:\n",
        "\n",
        "* Ø§Ø² **decoder-only Transformer** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø¨Ø±Ø®Ù„Ø§Ù BERT Ú©Ù‡ encoder-only Ø§Ø³Øª).\n",
        "* ÙÙ‚Ø· Ø¨Ø§ **language modeling Ù…Ø¹Ù…ÙˆÙ„ÛŒ** Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ (Ø¨Ø¯ÙˆÙ† Ù…Ø§Ø³Ú© ÛŒØ§ NSP).\n",
        "* Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ… **WebText** Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡.\n",
        "* Ù‡ÛŒÚ† fine-tuning Ø®Ø§ØµÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯Ø› ÙÙ‚Ø· Ø§Ø² **prompting** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "\n",
        "### ðŸŒ ØªØ£Ø«ÛŒØ±:\n",
        "\n",
        "Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ Ú©Ù‡ ÛŒÚ© Ù…Ø¯Ù„ Ø¨Ø²Ø±Ú¯ Ø²Ø¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø¯ÙˆÙ† Ø¢Ù…ÙˆØ²Ø´ Ø§Ø®ØªØµØ§ØµÛŒØŒ **Ú†Ù†Ø¯ ÙˆØ¸ÛŒÙÙ‡ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ù‡Ù…â€ŒØ²Ù…Ø§Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯** (zero-shot, one-shot, few-shot).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”· 4. **GPT-3 (Brown et al., 2020) â€“ \"Language Models are Few-Shot Learners\"**\n",
        "\n",
        "### ðŸ§  Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ:\n",
        "\n",
        "> Ø¨Ø§ **Ø§ÙØ²Ø§ÛŒØ´ Ø´Ø¯ÛŒØ¯ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ø¯Ù„ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§**ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ few-shot Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø¯ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ fine-tuning.\n",
        "\n",
        "### ðŸ“Œ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:\n",
        "\n",
        "* 175 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ± (Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ø¯Ø± Ø²Ù…Ø§Ù† Ø®ÙˆØ¯).\n",
        "* Ù‡Ù…Ø§Ù† Ø³Ø§Ø®ØªØ§Ø± GPT-2 (decoder-only, auto-regressive) ÙˆÙ„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯â€ŒØªØ±.\n",
        "* ÙÙ‚Ø· Ø¨Ø§ prompting Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø³Ø§Ø¦Ù„ÛŒ Ù…Ø«Ù„ ØªØ±Ø¬Ù…Ù‡ØŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒØŒ Ú©Ø¯Ù†ÙˆÛŒØ³ÛŒØŒ Ùˆ... Ù‡Ø¯Ø§ÛŒØª Ú©Ø±Ø¯.\n",
        "* Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¬Ø¯Ø¯ (fine-tuning) Ù†ÛŒØ³Øª â€” ÙÙ‚Ø· Ù…Ø«Ø§Ù„ Ø¨Ø¯Ù‡ Ùˆ Ù…Ø¯Ù„ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
        "\n",
        "### ðŸŒ ØªØ£Ø«ÛŒØ±:\n",
        "\n",
        "Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ Ú©Ù‡ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ø¹Ø¸ÛŒÙ… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…Ù†Ø¹Ø·Ù** Ù…Ù†Ø¬Ø± Ø´ÙˆØ¯Ø› Ù†Ù‚Ø·Ù‡Ù” Ø¹Ø·Ù Ø¯Ø± Ø³Ø§Ø®Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ„Ø¯ Ø¹Ù…ÙˆÙ…ÛŒ Ù…Ø«Ù„ ChatGPT Ùˆ Copilot.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©ÙˆØªØ§Ù‡:\n",
        "\n",
        "| Ù…Ø¯Ù„         | Ø³Ø§Ø®ØªØ§Ø±            | Ù†ÙˆØ¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ                 | Ù‡Ø¯Ù               | Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø§ØµÙ„ÛŒ               |\n",
        "| ----------- | ----------------- | --------------------------- | ----------------- | ------------------------- |\n",
        "| Transformer | Encoder + Decoder | Supervised (ØªØ±Ø¬Ù…Ù‡)          | Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù¾Ø§ÛŒÙ‡       | Ù¾Ø§ÛŒÙ‡Ù” Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ    |\n",
        "| BERT        | Encoder-only      | Pretraining + Fine-tuning   | Ø¯Ø±Ú© Ø²Ø¨Ø§Ù†          | Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ Ù¾Ø§Ø³Ø®â€ŒØ¨Ù‡â€ŒØ³ÙˆØ§Ù„   |\n",
        "| GPT-2       | Decoder-only      | ÙÙ‚Ø· Pretraining             | ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†         | Ù…ØªÙ†â€ŒÙ†ÙˆÛŒØ³ÛŒØŒ multi-task     |\n",
        "| GPT-3       | Decoder-only      | ÙÙ‚Ø· Pretraining + Prompting | few-shot learning | ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ù†Ø¹Ø·ÙØŒ general AI |\n",
        "\n",
        "---\n",
        "\n",
        "Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§ÛŒØŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ùˆ ØªÙˆÛŒ Ø¬Ø¯ÙˆÙ„ PDF ÛŒØ§ Ù†Ù…ÙˆØ¯Ø§Ø± Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ù‡Ù… Ø¯Ø±Ø¨ÛŒØ§Ø±Ù…ØŒ ÛŒØ§ Ø®Ù„Ø§ØµÙ‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒâ€ŒØ´ÙˆÙ† Ø±Ùˆ Ù‡Ù… Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø¯Ù….\n"
      ],
      "metadata": {
        "id": "9n66c64aF1xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zi1rQlAgA3SN"
      }
    }
  ]
}