{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pip"
      ],
      "metadata": {
        "id": "BmNi09ugEJGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e41oSpy2Dw2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6579ebf8-ca63-405f-a5ad-192d0083de86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "4q_rusnnEE2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "iTesZffCEojB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "import psutil\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import mode\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, decoders, processors\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, Dataset, IterableDataset, DataLoader"
      ],
      "metadata": {
        "id": "SdiamMUaEQzu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "shell = get_ipython()\n",
        "\n",
        "def adjust_font_size():\n",
        "  display(HTML('''<style>\n",
        "    body {\n",
        "      font-size: 24px;\n",
        "    }\n",
        "  '''))\n",
        "\n",
        "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
        "  shell.events.register('pre_execute', adjust_font_size)"
      ],
      "metadata": {
        "id": "yPykqb6Q9UNf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwaY_YcgRayy"
      },
      "source": [
        "# 🔴 **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokens, seq_len):\n",
        "    # Trim tokens so that total length is divisible by seq_len\n",
        "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
        "    tokens = tokens[:n_tokens]\n",
        "\n",
        "    # Reshape to 2D tensor\n",
        "    return tokens.view(-1, seq_len)\n"
      ],
      "metadata": {
        "id": "Vp_t8qCRfNCx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "88c0c982-1ec7-4046-e834-fe0f85e3e2d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PpKbTUEIRayz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "be961f7a-72c5-494e-9097-8b76de469106"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def num_trainable_params(model):\n",
        "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_time(model, x, num_runs=10):\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(*x)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.time() - start) / num_runs\n",
        "\n",
        "\n",
        "def calculate_time_cpu(model, x, num_runs=10):\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(*x)\n",
        "    return (time.time() - start) / num_runs"
      ],
      "metadata": {
        "id": "G6LDfZvmOLcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "887545bb-41f4-449e-d9b5-cc7570bef9e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟥 tokenize Tiktoken fast"
      ],
      "metadata": {
        "id": "ocpBR9r9E0Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "RUTUzg-pGeYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e05c352-abff-4d88-c429-df0c58da7b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvHCVoih5Bap",
        "outputId": "66ba7e09-0fc3-48e3-e12a-6596a9d12f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (2119719, 1), 'validation': (21990, 1)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "tokenized_train_samples = []\n",
        "for item in tqdm(dataset[\"train\"], desc=\"Tokenizing Train Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"])\n",
        "    tokenized_train_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "id": "n9l5SC43E5zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_valid_samples = []\n",
        "for item in tqdm(dataset[\"validation\"], desc=\"Tokenizing validation Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"])\n",
        "    tokenized_valid_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "id": "LrN9LZA03mL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_valid_samples[:1]"
      ],
      "metadata": {
        "id": "cc-VMQ0251vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sumtoks=  sum(len(tok) for tok in tokenized_train_samples)\n",
        "print(sumtoks)"
      ],
      "metadata": {
        "id": "EEIO6-sDIUqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟥 Train Bpe Tokenizer and data loader"
      ],
      "metadata": {
        "id": "FfNEUe3ss4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟧 BPE Trainer"
      ],
      "metadata": {
        "id": "CMYf01JHj07X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"|<unk>|\"))\n",
        "\n",
        "# Use a pre-tokenizer to split text into words\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# Initialize a BPE trainer\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=10_000,  # Set the vocabulary size\n",
        "    special_tokens=[\"|<unk>|\", \"<|endoftext|>\"],\n",
        "    min_frequency=2,  # Set the minimum frequency of tokens\n",
        "    )\n",
        "\n",
        "# Train the tokenizer on a custom dataset\n",
        "tokenizer.train_from_iterator(dataset[\"train\"][\"text\"], trainer)\n",
        "\n",
        "# Add special tokens\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"<|endoftext|> $A\",\n",
        "    special_tokens=[(\"<|endoftext|>\", tokenizer.token_to_id(\"<|endoftext|>\"))],\n",
        ")\n",
        "\n",
        "# Add decoder\n",
        "tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# Save the trained tokenizer\n",
        "tokenizer.save(\"bpe-tokenizer_tinystories.json\")\n",
        "\n",
        "#\n",
        "print(f\"🎉 Tokenizer training complete!\")\n",
        "print(f\"🔹 Vocabulary size: {tokenizer.get_vocab_size():,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0FFXwS3tf9l",
        "outputId": "8d15b6dd-aa76-4fc8-90ad-bb33aa956897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Tokenizer training complete!\n",
            "🔹 Vocabulary size: 10,000 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
        "print(f\"🎉 Tokenizer training complete!\")\n",
        "print(f\"🔹 Vocabulary size: {tokenizer.get_vocab_size():,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIB3W1p53lOc",
        "outputId": "2a50cb8d-f64b-4b18-f195-d29d7ba8f630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Tokenizer training complete!\n",
            "🔹 Vocabulary size: 10,000 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'They played together all day and became best friends.'\n",
        "tokens = tokenizer.encode(sent)\n",
        "print(tokens.ids)\n",
        "print(tokens.tokens)\n",
        "\n",
        "pprint(tokenizer.decode(tokens.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "798_gzXZuM5x",
        "outputId": "3d103108-d7b1-4500-b626-1e9db520e12d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 546, 667, 462, 378, 252, 161, 1042, 725, 375, 15]\n",
            "['<|endoftext|>', 'They', 'Ġplayed', 'Ġtogether', 'Ġall', 'Ġday', 'Ġand', 'Ġbecame', 'Ġbest', 'Ġfriends', '.']\n",
            "'They played together all day and became best friends.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟧 Save and load Tokens with BPE tokenizer"
      ],
      "metadata": {
        "id": "H0gmGOkqIAxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization {train}\n",
        "tokenized_train_samples = []\n",
        "for item in tqdm(dataset[\"train\"], desc=\"Tokenizing Train Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"]).ids\n",
        "    tokenized_train_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "id": "NdaDekSJuq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_samples_concat=[]\n",
        "tokenized_train_samples_concat = np.concatenate(tokenized_train_samples)\n",
        "len(tokenized_train_samples_concat)"
      ],
      "metadata": {
        "id": "hi9uU-62Nq21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokens as a pytorch file\n",
        "torch.save(torch.tensor(tokenized_train_samples_concat), 'tokenized-train-samples_vocab-10k.pt')"
      ],
      "metadata": {
        "id": "rt_1F7qvQcfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization {validation}\n",
        "tokenized_valid_samples = []\n",
        "for item in tqdm(dataset[\"validation\"], desc=\"Tokenizing Validation Set\"):\n",
        "    input_ids = tokenizer.encode(item[\"text\"]).ids\n",
        "    tokenized_valid_samples.append(np.array(input_ids))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "On0JGjWju6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_valid_samples_concat=[]\n",
        "tokenized_valid_samples_concat = np.concatenate(tokenized_valid_samples)\n",
        "len(tokenized_valid_samples_concat)"
      ],
      "metadata": {
        "id": "Kz6unOJpAshB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokens as a pytorch file\n",
        "torch.save(torch.tensor(tokenized_valid_samples_concat), 'tokenized-valid-samples_vocab-10k.pt')"
      ],
      "metadata": {
        "id": "DB6a7nNC8_tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟧 Custom dataset"
      ],
      "metadata": {
        "id": "6c2dBVc4wf-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyStoriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.seq_len = seq_len\n",
        "        self.data = prepare_data(data, seq_len+1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample[:-1], sample[1:]"
      ],
      "metadata": {
        "id": "1oQvaIZLwkIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "118c7046-051b-4ed9-fae4-2fe51df0715c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = TinyStoriesDataset(tokenized_train_samples, 128)\n",
        "train_set.data.shape, len(train_set), train_set[0]"
      ],
      "metadata": {
        "id": "ZByVGGuUwenF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "bdc2a840-a69c-4d5d-cd93-71ebef3853e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-be0809126efa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyStoriesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d054a2508118>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, seq_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ff7d104d76c7>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(tokens, seq_len)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Trim tokens so that total length is divisible by seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mn_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit next(iter(train_set))"
      ],
      "metadata": {
        "id": "k47DvjFVxOdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=next(iter(train_set))"
      ],
      "metadata": {
        "id": "U0SjBcbtatUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(b)"
      ],
      "metadata": {
        "id": "6XCHXYbaaxPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟧 DataLoader"
      ],
      "metadata": {
        "id": "x2lJvLkFx-Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "ZrG3JwDrg_9e",
        "outputId": "150f6f95-1abc-4cca-e48d-e391335b5fc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/temp/bpe-tokenizer_tinystories.json\")\n",
        "tokenizer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a700lJAToJ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_train_samples = torch.load('/content/drive/MyDrive/temp/tokenized-train-samples_vocab-10k.pt')\n",
        "tokenized_valid_samples = torch.load('/content/drive/MyDrive/temp/tokenized-valid-samples_vocab-10k.pt')\n",
        "\n",
        "# train_set = TinyStoriesDataset(tokenized_train_samples, seq_len=128)\n",
        "valid_set = TinyStoriesDataset(tokenized_valid_samples, seq_len=128)"
      ],
      "metadata": {
        "id": "F5evvjhex63m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9a5fa676-17e5-4d20-8547-285fcc0389db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = DataLoader(train_set, batch_size=32, shuffle=True, pin_memory=True) #, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, pin_memory=True) #, num_workers=2)"
      ],
      "metadata": {
        "id": "0Acjf4UeyMHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e4f0d595-24c5-4e4d-b914-07cd3a5b38cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = next(iter(valid_loader))\n",
        "x_batch.shape, y_batch.shape"
      ],
      "metadata": {
        "id": "qkhNsYnMyUwe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "75e3c4ec-a4f9-42e9-d378-442ecbef505a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 128]), torch.Size([32, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_batch[0,:])\n",
        "print('\\n',y_batch[0,:])"
      ],
      "metadata": {
        "id": "YdOkfGMPnnHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(valid_loader)"
      ],
      "metadata": {
        "id": "4Em6sZ_syl2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader) / (20*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICs47WHsftyW",
        "outputId": "73583e6d-7b03-4b65-eaba-b67e11995bba"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.865"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = iter(train_loader)"
      ],
      "metadata": {
        "id": "ihV-0spZfZhq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit next(train_iter)"
      ],
      "metadata": {
        "id": "_RMcGk_qyojK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟧 EDA"
      ],
      "metadata": {
        "id": "wMtdHFsCN2Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_count_stories=[]\n",
        "for tokns in tokenized_train_samples:\n",
        "    token_count_stories.append(len(tokns))"
      ],
      "metadata": {
        "id": "UkQNex4eDn1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "106e2e8e-5632-412a-a1eb-4fce8a3b7143"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_count_stories_np=np.array(token_count_stories)"
      ],
      "metadata": {
        "id": "cxWO9YTOENR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(token_count_stories, bins=50, kde=True)\n",
        "plt.xlabel('Token Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Token Counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JRcUm7utIHkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sort(token_count_stories_np)[:1000]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LAGEzHvoN-Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟥 Transformer Model from scratch"
      ],
      "metadata": {
        "id": "vOQNBqhXMuYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtention(torch.nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv_proj = torch.nn.Linear(embed_dim, 3 * embed_dim)\n",
        "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
        "    # احتمالا خطا ابعاد دارد زمان ترین ممکن است ترین نشود\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        k,q,v = self.qkv_proj(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim).transpose(1,2).chunk(3)\n",
        "        # F.scaled_dot_product_attention(q,k,v)\n",
        "        # return self.out_proj(x)\n",
        "        return q"
      ],
      "metadata": {
        "id": "E7h7rW2dDss8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6e6d73a5-1530-4303-9c3a-68f49ef99d08"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.range(1,24).view(2,3,4)\n",
        "print(x)\n",
        "\n",
        "# /x=x.transpose(1,0)\n",
        "print(x)\n",
        "#"
      ],
      "metadata": {
        "id": "cZ0QUKKhQ8hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "y= MultiHeadAtention(4,2)(x)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "a0pQHFFuRGNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 **Model from scratch - Howsam**"
      ],
      "metadata": {
        "id": "ZnBQs-C8PNrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Define"
      ],
      "metadata": {
        "id": "oIjl-VOXpzAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "TBwuEJI6NNNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9b3f34fc-bad6-4577-fc3e-b190ce5cc6c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "VuXbNcH1p7mv",
        "outputId": "7f6e6d9d-6f29-4fef-e73f-772ec9ba8416"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Embedding"
      ],
      "metadata": {
        "id": "oP9PxAiBOXG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wte = nn.Embedding(tokenizer.get_vocab_size(), 100)\n",
        "wte(torch.tensor([1, 2,3])).shape"
      ],
      "metadata": {
        "id": "D3KuSAfszsHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "f00d94b9-1304-40c5-ae63-8c6bedc0de03"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128\n",
        "wpe = nn.Embedding(seq_len, 100)\n",
        "wpe(torch.tensor([1, 2, 100])).shape"
      ],
      "metadata": {
        "id": "NN6B1P0tzsBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "e38602bc-b3e5-438e-de99-5a6b510c327f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wpe(torch.arange(x_batch.shape[1])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "qWFIHM3x-26z",
        "outputId": "228fda44-2e3c-4f5b-cbd6-2fce909e4151"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = wte(x_batch) + wpe(torch.arange(x_batch.shape[1]))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "AMgXEhAwIJxj",
        "outputId": "dd0aa91d-9897-4f9d-e5aa-c88405578c31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 128, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "CHJ4NWXGR0YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = k = v = x\n",
        "print(q.shape)\n",
        "\n",
        "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "\n",
        "scores = q @ k.transpose(-2, -1) / (k.shape[-1]**0.5)\n",
        "scores.masked_fill_(mask ==0, float(-torch.inf))\n",
        "scores = scores.softmax(dim=-1)\n",
        "print(scores.shape)\n",
        "\n",
        "z = scores @ v\n",
        "z.shape"
      ],
      "metadata": {
        "id": "9j_qy0aIJZXl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "acaabdf9-22fe-4d4d-b0c3-3d5e0a726411"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-63054fa8b2ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = torch.randn(3, 5, 5)\n",
        "# mask = torch.tril(torch.ones(5, 5))\n",
        "# scores.masked_fill_(mask ==0, float(-torch.inf))\n",
        "# scores = scores.softmax(dim=-1)\n",
        "# scores"
      ],
      "metadata": {
        "id": "QptCjesZL96l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v):\n",
        "    mask = torch.tril(torch.ones(q.shape[-2], q.shape[-2])).to(device)\n",
        "    scores = q @ k.transpose(-2, -1) / (k.shape[-1]**0.5)\n",
        "    scores.masked_fill_(mask==0, float(-torch.inf))\n",
        "    scores = scores.softmax(dim=-1)\n",
        "    z = scores @ v\n",
        "    return z"
      ],
      "metadata": {
        "id": "mIEMC-FarKJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product_attention(x.to(device), x.to(device), x.to(device)).shape"
      ],
      "metadata": {
        "id": "wdt56H82PQqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn((128, 1024, 768), device=device)\n",
        "k = torch.randn((128, 1024, 768), device=device)\n",
        "v = torch.randn((128, 1024, 768), device=device)\n",
        "q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "xK6XgVlePa0n",
        "outputId": "fd64b71c-d265-445d-b77a-5abd363a73d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1024, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product_attention(q, k, v).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5COTL0OKPxY1",
        "outputId": "46cc32a9-f255-4508-9fc3-9f1cbf376790"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scaled_dot_product_attention' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d35194045c40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'scaled_dot_product_attention' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate_time(scaled_dot_product_attention, (q, k, v), num_runs=20)\n",
        "calculate_time_cpu(scaled_dot_product_attention, (q, k, v), num_runs=20)"
      ],
      "metadata": {
        "id": "LsaSSCkaQLtw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.scaled_dot_product_attention(q, k, v, is_causal=True).shape"
      ],
      "metadata": {
        "id": "UmCJ2BV6Unge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(scaled_dot_product_attention(q, k, v) - F.scaled_dot_product_attention(q, k, v, is_causal=True)).max()"
      ],
      "metadata": {
        "id": "IjrAqzTWU79x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(F.scaled_dot_product_attention, (q, k, v), num_runs=20)"
      ],
      "metadata": {
        "id": "cnGXNViJVWzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Multi Head Attention"
      ],
      "metadata": {
        "id": "rUjFcYeiIE9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MultiHeadAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(100, 1000)\n",
        "#         self.fc2 = nn.Linear(1000, 100)\n",
        "#         self.fc3 = nn.Linear(1000, 100)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         y = F.relu(self.fc1(x))\n",
        "#         y1 = self.fc2(y)\n",
        "#         y2 = self.fc3(y)\n",
        "#         return F.relu(torch.concat([y1, y2], dim=-1))"
      ],
      "metadata": {
        "id": "gnxYy1oIQcY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "601d8702-3d97-430e-affc-d934c8994ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mha = MultiHeadAttention()\n",
        "# num_trainable_params(mha)\n",
        "# mha.forward(torch.rand(10, 100)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Et6wVx6f7HmB",
        "outputId": "3271b803-66f5-40db-80b8-de54a91a19df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x=torch.randn(2,4)\n",
        "# print(x)\n",
        "# lx=nn.Linear(4,8,bias=False)\n",
        "# y1 =x@ lx.weight.T\n",
        "# y2=lx(x)\n",
        "# print(y1.softmax(dim=-1).argmax(dim=0))\n",
        "# print(y1.softmax(dim=-1))\n",
        "# print(lx.weight.T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7FtEClDCBhKQ",
        "outputId": "b869524b-2324-415f-8448-eeb4dbad62cf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    n_embd: int = 100\n",
        "    n_head: int = 5\n",
        "\n",
        "config = GPTConfig()\n",
        "config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "q10jLOLsuzxi",
        "outputId": "bddc5cde-3f52-442d-d0ae-9c7326dbc2c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.head_size = self.n_embd // self.n_head\n",
        "\n",
        "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
        "\n",
        "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.c_proj.residual = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y #,q, k, v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2X9b79Iq8l38",
        "outputId": "c26667b0-ccc4-4568-b722-3dfb19fd62f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mha = MultiHeadAttention(config)\n",
        "# y,q, k, v= mha(x)\n",
        "# print(\"X:\",x.shape)\n",
        "# print(\"qkv_proj:\",mha.qkv_proj.weight.T.shape)\n",
        "# print(\"c_proj:\",mha.c_proj.weight.T.shape)\n",
        "\n",
        "# print(\"q:\",q.shape)\n",
        "# print(k.shape)\n",
        "# print(v.shape)\n",
        "# print(\"y:\",y.shape)"
      ],
      "metadata": {
        "id": "vA3Nw2Riv1Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(mha.to(device), (x.to(device),), num_runs=20)"
      ],
      "metadata": {
        "id": "3YRyw8Hu-bXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Feed Forward (MLP)"
      ],
      "metadata": {
        "id": "WTVewgG-m7nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    n_embd: int = 100\n",
        "    n_head: int = 5\n",
        "    f_expnd: float = 4\n",
        "\n",
        "config = GPTConfig()\n",
        "config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "R-r7XatDAP4B",
        "outputId": "824fd999-ea8d-4728-f0c4-cdb387039d14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.f_expnd = config.f_expnd\n",
        "\n",
        "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
        "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
        "        self.down_proj.residual = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(F.gelu(self.up_proj(x)))"
      ],
      "metadata": {
        "id": "BTcx4J5Lm66z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "09a70790-ae3e-42d4-edd0-41b31f7e5712"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x1=torch.randn(2,4,100)\n",
        "# plt.hist(x1.flatten(), bins=50);\n",
        "# plt.show()\n",
        "# plt.hist(F.gelu(x1).flatten(), bins=50);\n",
        "# plt.show()\n",
        "# F.gelu(x1).min(dim=-1)"
      ],
      "metadata": {
        "id": "aJUGrkOkm6tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feedfor = FeedForward(config)\n",
        "print(feedfor(x).shape)\n",
        "feedfor.up_proj.weight.T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "aDU2oB-PAfXX",
        "outputId": "41400bfa-5b7f-408d-c919-db9af8cc8e35"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-95be6d2b032e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeedfor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedfor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfeedfor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params(feedfor)*1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "JtLY269MBHD3",
        "outputId": "37d39567-148e-4711-c783-0b2fa4dcc63c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(mlp, (x, ), num_runs=20)"
      ],
      "metadata": {
        "id": "FKYxPvrzBQbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Decoder Block"
      ],
      "metadata": {
        "id": "ih6sV9ljndzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.mha = MultiHeadAttention(config)\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gq1kw31av9DR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a9fd3ac9-ec60-4e74-de75-8dbb454660cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = DecoderBlock(config)\n",
        "y = decoder(x)\n",
        "print(x.shape)\n",
        "plt.hist(x.detach().flatten(), bins=50);\n",
        "plt.show()\n",
        "plt.hist(y.detach().flatten(), bins=50);\n",
        "plt.show()\n",
        "\n",
        "means_before = x.mean(dim=-1)  # shape: (32, 128)\n",
        "means_after = y.mean(dim=-1)\n",
        "\n",
        "stds_before = x.std(dim=-1)\n",
        "stds_after = y.std(dim=-1)\n",
        "\n",
        "plt.hist(means_before.detach().flatten(), bins=50);\n",
        "plt.show()\n",
        "plt.hist(means_after.detach().flatten(), bins=50);\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "AweCYj03EtPy",
        "outputId": "2ae361de-3662-4a22-f409-9fa1104a7e32"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-061b9b612b08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norml=nn.LayerNorm(100)\n",
        "xrnd=torch.range(1,100)\n",
        "print(xrnd.min().item(), xrnd.max().item(), xrnd.mean().item(), xrnd.std().item())\n",
        "print(norml(xrnd).min().item(), norml(xrnd).max().item(),norml(xrnd).mean().item(),9 , norml(xrnd).std().item())\n",
        "plt.hist(xrnd, bins=10);\n",
        "plt.show()\n",
        "plt.hist(norml(xrnd).detach(), bins=10);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TJdKmDWRyf2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params(decoder) * 1e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "pO5KeXeLFmDk",
        "outputId": "23b8ff5e-e9d1-4c6c-efff-a8b3449dfb28"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'decoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-202477f16d8e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_trainable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time_cpu(decoder, (x, ), num_runs=20) * 1e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "7zAYllgcFsfg",
        "outputId": "88afe093-aaf2-4a2a-9702-9a8ed1a2f320"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.37346315383911"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 GPT"
      ],
      "metadata": {
        "id": "LCzTHf8iitEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    vocab_size: int = 10_000\n",
        "    seq_len: int = 128\n",
        "    n_layer: int = 12\n",
        "    n_embd: int = 100\n",
        "    n_head: int = 5\n",
        "    f_expnd: float = 4\n",
        "\n",
        "\n",
        "config = GPTConfig()\n",
        "config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "adRz9KC8HbbT",
        "outputId": "a9141e40-f01f-4c39-d8db-3be4538bdd53"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.seq_len, config.n_embd)\n",
        "        # self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "        self.lnf = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.lm_head.weight = self.wte.weight\n",
        "        # self.lm_head.weight.data.uniform_(-1/self.lm_head.in_features**0.5, 1/self.lm_head.in_features**0.5)\n",
        "        # nn.init.uniform_(self.lm_head.weight, -1/self.lm_head.in_features**0.5, 1/self.lm_head.in_features**0.5)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'residual'):\n",
        "                std *= (2*self.config.n_layer)**-0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
        "\n",
        "        # x = self.decoders(x)\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x)\n",
        "\n",
        "        x = self.lnf(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Lmrc034JwvSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "240f1812-4400-40d2-8d60-fdb72e6975c2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(config).to(device)\n",
        "logits= model(x_batch.to(device))\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "FwYwWZ_Z-dV-",
        "outputId": "8bc0928d-0875-469b-8d9a-0819c4586dd5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FeedForward' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e4d096bf88f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-ba613d993dcb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-ba613d993dcb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-1cf5dd95b63b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FeedForward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🦾 تمرین:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "بخش تولید متن را جنریت و برای مدل جی پی تی به شکلی ساده و بر اساس آرگ مکس پیاده‌سازی کنید"
      ],
      "metadata": {
        "id": "Tf7SiFsgp99a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_tokens= torch.argmax(F.softmax(logits,dim=-1), dim=-1)\n",
        "pprint(tokenizer.decode(x_batch[0,:].tolist()))\n",
        "print('\\n\\n')\n",
        "pprint(tokenizer.decode(gen_tokens[0,:].tolist()))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "BEcruFtckoln",
        "outputId": "b9158436-bf2e-4340-e253-127f2c530a96"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'logits' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-67f18321f891>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_tokens\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "novLGFNRkUYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.lm_head.weight.T.shape , model.wte.weight.T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "CWsuN52NNtYV",
        "outputId": "700b9330-4634-49c7-cc2b-7029aeeaa15d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d0ae9be3fc86>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trainable_params(model), num_trainable_params(model.decoders), num_trainable_params(model.lm_head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "JdecDmI3-rvN",
        "outputId": "a7573aa5-f30d-4169-f4f1-331044894992"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.4578, 1.4448, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_time(model, (x_batch.to(device),), num_runs=100) * 1e3"
      ],
      "metadata": {
        "id": "gxIo0WS7-z9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Initialization"
      ],
      "metadata": {
        "id": "KEnoziDiCZ84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(\n",
        "    GPTConfig).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UVGTRYO9FH3O",
        "outputId": "c0fcbc55-40f0-4ec9-972e-22757ea4cb3d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model.decoders[0].mha.c_proj.weight.flatten().detach().cpu(), bins=50);"
      ],
      "metadata": {
        "id": "dy1ErMbaCeNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.02 * (2*4)**-0.5 * 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "OnIIvdoUIjjO",
        "outputId": "d22bef45-2688-4ac2-f231-2d9ad77406e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021213203435596427"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model.wpe.weight.flatten()[:100_000].detach().cpu(), bins=50);"
      ],
      "metadata": {
        "id": "l10kLsGcFzAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model)"
      ],
      "metadata": {
        "id": "ZAbJNo57IyP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(model.decoders[2].mlp.down_proj.weight.flatten().detach().cpu(), bins=50);"
      ],
      "metadata": {
        "id": "DB-vyDHpI28x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟥 GPT model implement with nn.torch transformer"
      ],
      "metadata": {
        "id": "uVkN_q8j7Dpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🦾 تمرین\n",
        "\n",
        "\n",
        "شبکه جی پی تی را با استفاده از دستورات آماده پای تورچ پیاده‌سازی کنید و زمان اجرای آن را با مدل پیاده‌سازی‌شده از صفر مقایسه کنید."
      ],
      "metadata": {
        "id": "y0lYZZAw5rw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = torch.nn.Transformer(d_model=100, nhead=5, num_encoder_layers=0, num_decoder_layers=12, dim_feedforward=400,\n",
        "                     dropout=0.1, activation=\"gelu\", custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05,\n",
        "                     batch_first=False, norm_first=False, bias=True, device=None, dtype=None)"
      ],
      "metadata": {
        "id": "8lp68U6i7gkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "f351958b-0702-49a1-a618-fe8e6ba55adc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_layer = nn.TransformerDecoderLayer(d_model= 100, nhead=5, dim_feedforward=400)\n",
        "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=12)\n",
        "memory = torch.rand(10, 32, 100)\n",
        "tgt = torch.rand(20, 32, 100)\n",
        "out = transformer_decoder(tgt, memory)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "id": "nzcZJ2jss9e2",
        "outputId": "59f119a7-4620-47aa-f948-3328f037f116"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTTorch(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.seq_len, config.n_embd)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model= config.n_embd, nhead=config.n_head,\n",
        "                                                   dim_feedforward=config.f_expnd*config.n_embd,activation=F.gelu,norm_first=True\n",
        "                                                   )\n",
        "        # decoder_layer.self_attn.is_causal = True\n",
        "        self.decoders = nn.TransformerDecoder(decoder_layer, num_layers=12)\n",
        "        # self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "        self.lnf = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.lm_head.weight = self.wte.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'residual'):\n",
        "                std *= (2*self.config.n_layer)**-0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
        "\n",
        "        # x = self.decoders(x)\n",
        "        # for decoder in self.decoders:\n",
        "        x = self.decoders(x ,memory=None)\n",
        "\n",
        "        x = self.lnf(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "58fa192f-25d7-47fb-a820-b543d3374d3a",
        "id": "6C0k8lUSwAKT"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTTorch(config).to(device)\n",
        "logits= model(x_batch.to(device))\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "2_2QuMoa1KXV",
        "outputId": "58549a8f-7c7d-4ce2-ee2f-d0338ef88b95"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'is_nested'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-3993b6555f2c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-6ef006ed7e03>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# x = self.decoders(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# for decoder in self.decoders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlnf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    614\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             )\n\u001b[0;32m-> 1098\u001b[0;31m             x = x + self._mha_block(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mis_causal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     ) -> Tensor:\n\u001b[0;32m-> 1148\u001b[0;31m         x = self.multihead_attn(\n\u001b[0m\u001b[1;32m   1149\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                     )\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0many_nested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m         assert not any_nested, (\n\u001b[1;32m   1331\u001b[0m             \u001b[0;34m\"MultiheadAttention does not support NestedTensor outside of its fast path. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_nested'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_tokens= torch.argmax(F.softmax(logits,dim=-1), dim=-1)\n",
        "pprint(tokenizer.decode(x_batch[0,:].tolist()))\n",
        "print('\\n\\n')\n",
        "pprint(tokenizer.decode(gen_tokens[0,:].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "wRd4kbT91YSR",
        "outputId": "f2c681a0-5fce-40e5-f1d3-9361c81ba7fa"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright '\n",
            " 'and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every '\n",
            " 'day.\"\\n'\n",
            " '\\n'\n",
            " 'After playing with the car, Kitty and Spot felt thirsty. They found a small '\n",
            " 'pond with clear water. They drank the water and felt very happy. They played '\n",
            " 'together all day and became best friends.Once upon a time, in a big forest, '\n",
            " 'there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, '\n",
            " 'rocks, and hills. One day, Roxy found an icy hill. She')\n",
            "\n",
            "\n",
            "\n",
            "(' throws singer becoming disgust mouse disgust disgust His earn farm ideas '\n",
            " 'disgustothyextext farmendant farm farm farm farmendantendantendant opera smo '\n",
            " 'farm glided intelligink donâiments intellig smo smoimentsœWhereœWhere '\n",
            " 'plantroom att bru rang rang donâ donâ att successful promisedext meowing '\n",
            " 'slower as blood h att blood hhen cuts h htedink hhenhenMaya exc beetle att '\n",
            " 'slower gap h h Carl asounced hawkhenhenlephantext aston wriggled blood h '\n",
            " 'chairs paintinghen comple activitiesext hours painting pistol mittens '\n",
            " 'hextlephant blood emergencyext argumBob dragext Avaexthog line '\n",
            " 'hayMaddieTruelinghenext� tant miracle h natural pout teach teach charm teach '\n",
            " 'teach')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🦾 تمرین\n",
        "\n",
        "کانفیگ مدل‌های تاینی استوری را در یک جدول خلاصه کنید و براساس این کانفیگ‌ها مدل را بسازید.\n",
        "\n",
        "Howsam |vocab_size:10_000| seq_len:128| n_layer:12| n_embd:100|n_head:5| f_expnd: 4  \n",
        "\n",
        "GPT-14 |vocab_size:50304| seq_len:1024| n_layer:12| n_embd:768|n_head:12| f_expnd: 4| dropout: 0.0| bias:True\n",
        "\n",
        "gpt_neo | vocab_size:50257| seq_len:256| n_layer:8| n_embd:256|n_head:16| f_expnd: --| dropout: 0.1| torch_dtype:float16\n"
      ],
      "metadata": {
        "id": "EThI0ZACnJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🦾  تمرین\n",
        "\n",
        "\n",
        "البته! در اینجا خلاصه‌ای از **ایده‌های اصلی** سه مدل معروف از دو خانواده مختلف آورده شده است:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 خانواده Encoder-only (مدل‌هایی مثل BERT)\n",
        "\n",
        "### 📌 1. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "\n",
        "* **نوع**: Encoder-only\n",
        "* **ایده اصلی**: یادگیری نمایش‌های عمیق زبانی از متن با خواندن **دو طرفه** (bidirectional).\n",
        "* **روش آموزش**:\n",
        "\n",
        "  * **Masked Language Modeling (MLM)**: بعضی از توکن‌ها در جمله ماسک می‌شوند، و مدل باید آن‌ها را پیش‌بینی کند.\n",
        "  * **Next Sentence Prediction (NSP)**: آیا جمله دوم دنباله جمله اول هست یا نه؟\n",
        "* **کاربردها**: درک زبان، طبقه‌بندی متن، Named Entity Recognition و ...\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**\n",
        "\n",
        "* **نوع**: Encoder-only (مبتنی بر BERT)\n",
        "* **ایده اصلی**: بهبود کیفیت آموزش BERT با حذف NSP، استفاده از داده بیشتر و batchهای بزرگ‌تر.\n",
        "* **تفاوت با BERT**:\n",
        "\n",
        "  * فقط از MLM استفاده می‌کند.\n",
        "  * از shuffling بهتر داده‌ها و dynamic masking بهره می‌برد.\n",
        "* **کاربردها**: مشابه BERT اما با دقت بالاتر در بسیاری از تست‌ها.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 3. **ELECTRA**\n",
        "\n",
        "* **نوع**: Encoder-only (ولی با مکانیزم متفاوت)\n",
        "* **ایده اصلی**: به‌جای پیش‌بینی توکن‌های ماسک‌شده (مانند BERT)، یاد می‌گیرد تشخیص دهد که **آیا یک توکن واقعی است یا جایگزین شده**.\n",
        "* **روش آموزش**:\n",
        "\n",
        "  * یک generator توکن‌های جعلی تولید می‌کند.\n",
        "  * یک discriminator یاد می‌گیرد تشخیص دهد کدام توکن‌ها تغییر کرده‌اند.\n",
        "* **مزیت**: سریع‌تر و کارآمدتر از BERT از نظر یادگیری.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 خانواده Decoder-only (مدل‌هایی مثل GPT)\n",
        "\n",
        "### 📌 1. **GPT-2**\n",
        "\n",
        "* **نوع**: Decoder-only\n",
        "* **ایده اصلی**: مدل‌سازی زنجیره‌ای زبان به‌صورت **چپ به راست** (causal), با پیش‌بینی توکن بعدی.\n",
        "* **روش آموزش**:\n",
        "\n",
        "  * فقط از **Language Modeling (Auto-regressive)** استفاده می‌کند.\n",
        "* **کاربردها**: تولید متن، خلاصه‌سازی، پاسخ به پرسش، ترجمه و ...\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 2. **GPT-3**\n",
        "\n",
        "* **نوع**: Decoder-only (توسعه‌یافته‌تر از GPT-2)\n",
        "* **ایده اصلی**: مقیاس‌دهی شدید (175B پارامتر) + توانایی انجام چندوظیفه‌ای بدون fine-tuning.\n",
        "* **ویژگی خاص**: یادگیری از طریق نمونه (Few-shot / Zero-shot Learning).\n",
        "* **کاربردها**: چت‌بات، برنامه‌نویسی خودکار، مقاله‌نویسی و...\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 3. **LLaMA (Large Language Model Meta AI)**\n",
        "\n",
        "* **نوع**: Decoder-only\n",
        "* **ایده اصلی**: طراحی مدل‌های سبک‌تر اما بسیار کارآمد (معمولاً با پارامترهای کمتر از GPT-3 اما عملکرد مشابه).\n",
        "* **مزیت**: باز بودن مدل‌ها و توانایی اجرا روی سخت‌افزار محدود.\n",
        "* **کاربردها**: چت‌بات، تحقیقاتی، ابزارهای پردازش زبان باز.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ جمع‌بندی جدول‌وار:\n",
        "\n",
        "| مدل     | نوع معماری   | روش یادگیری اصلی           | کاربرد کلیدی                |\n",
        "| ------- | ------------ | -------------------------- | --------------------------- |\n",
        "| BERT    | Encoder-only | MLM + NSP                  | درک زبان                    |\n",
        "| RoBERTa | Encoder-only | MLM (بهینه‌شده)            | طبقه‌بندی و درک پیشرفته     |\n",
        "| ELECTRA | Encoder-only | Discriminator-Generator    | آموزش سریع‌تر و دقیق‌تر     |\n",
        "| GPT-2   | Decoder-only | Auto-regressive LM         | تولید متن                   |\n",
        "| GPT-3   | Decoder-only | Few-shot + Auto-regressive | پاسخ‌دهی چندکاره            |\n",
        "| LLaMA   | Decoder-only | Efficient auto-regressive  | تحقیقاتی و کاربردهای سبک‌تر |\n",
        "\n",
        "---\n",
        "\n",
        "اگر خواستی مدل‌هایی از خانواده Encoder-Decoder مثل T5 یا BART رو هم بررسی کنیم، فقط بگو.\n"
      ],
      "metadata": {
        "id": "3efgNYrZ3u77"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86bxO68Z32Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🦾 تمرین\n",
        "\n",
        "با چه تغییراتی می‌توان GPT را به مدلی مشابه Llama 3.2 تبدیل کرد؟ ایده‌های خود را بیان کنید.\n",
        "\n",
        "این کد یکی از بخش‌های مهم **معماری مدل LLaMA 3** از شرکت **Meta AI** را پیاده‌سازی می‌کند. این معماری از نوع **Decoder-only** و بر پایه Transformer است، با بهینه‌سازی‌هایی مانند **Rotary Embeddings**، **مدیریت حافظه (KV Caching)**، و **مدل‌سازی موازی (Model Parallelism)** با استفاده از کتابخانه `fairscale`.\n",
        "\n",
        "بیایید آن را گام به گام و به زبان ساده تحلیل کنیم:\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 ساختار کلی کد\n",
        "\n",
        "| بخش                                                         | توضیح                                                        |\n",
        "| ----------------------------------------------------------- | ------------------------------------------------------------ |\n",
        "| `RMSNorm`                                                   | نرمال‌سازی RMS، جایگزینی سبک‌تر و پایدارتر برای LayerNorm    |\n",
        "| `apply_scaling`, `precompute_freqs_cis`, `apply_rotary_emb` | پیش‌پردازش موقعیت‌یابی چرخشی (RoPE)                          |\n",
        "| `Attention`                                                 | پیاده‌سازی کامل Attention با کش (KV cache) و پشتیبانی از GQA |\n",
        "| `FeedForward`                                               | شبکه FFN دو شاخه‌ای با سیگموید غیرخطی (SiLU)                 |\n",
        "| `TransformerBlock`                                          | یک بلوک کامل شامل Attention + FFN                            |\n",
        "| `Transformer`                                               | مدل کامل LLaMA شامل لایه‌های متعدد، embedding و خروجی نهایی  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 جزئیات مهم هر بخش\n",
        "\n",
        "### 🧠 1. `RMSNorm`\n",
        "\n",
        "```python\n",
        "class RMSNorm(nn.Module):\n",
        "```\n",
        "\n",
        "نرمال‌سازی «ریشه میانگین مربعات»، بدون کم‌کردن میانگین. این نرمال‌سازی سریع‌تر و پایدارتر است نسبت به LayerNorm.\n",
        "\n",
        "---\n",
        "\n",
        "### 🌀 2. `Rotary Embeddings`\n",
        "\n",
        "```python\n",
        "def apply_rotary_emb(xq, xk, freqs_cis): ...\n",
        "```\n",
        "\n",
        "ایده RoPE این است که موقعیت توکن‌ها را به‌صورت توابع سینوسی پیچشی وارد فضای attention کنیم. این باعث می‌شود اطلاعات ترتیبی (position) در dot-product attention لحاظ شود.\n",
        "\n",
        "* `precompute_freqs_cis` فرکانس‌ها را برای موقعیت‌ها پیش‌محاسبه می‌کند.\n",
        "* `apply_rotary_emb` این فرکانس‌ها را به query و key اعمال می‌کند.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡ 3. `Attention`\n",
        "\n",
        "```python\n",
        "class Attention(nn.Module): ...\n",
        "```\n",
        "\n",
        "لایه attention مدل با پشتیبانی از موارد زیر:\n",
        "\n",
        "* **GQA**: Grouped Query Attention (مثل LLaMA 2/3).\n",
        "* **Model Parallelism**: با `ColumnParallelLinear` و `RowParallelLinear`.\n",
        "* **KV Cache**: نگه‌داشتن key/value برای جلوگیری از محاسبه دوباره در طول decoding.\n",
        "* **Rotary Embeddings**: با `apply_rotary_emb` روی xq/xk.\n",
        "* **Causal Masking**: با `mask` در dot-product attention برای جلوگیری از نگاه به آینده.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ 4. `FeedForward`\n",
        "\n",
        "```python\n",
        "class FeedForward(nn.Module): ...\n",
        "```\n",
        "\n",
        "یک لایه FFN که ورودی را به hidden dimension گسترش می‌دهد و با سیگموید SiLU و ضرب pointwise خروجی تولید می‌کند. از ۳ لایه خطی استفاده شده (w1، w2، w3) برای افزایش انعطاف‌پذیری.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 5. `TransformerBlock`\n",
        "\n",
        "```python\n",
        "class TransformerBlock(nn.Module): ...\n",
        "```\n",
        "\n",
        "یک بلاک کامل شامل:\n",
        "\n",
        "* `RMSNorm → Attention → Residual`\n",
        "* `RMSNorm → FFN → Residual`\n",
        "\n",
        "با ترتیب مشابه معماری LLaMA (norm-first).\n",
        "\n",
        "---\n",
        "\n",
        "### 🏗️ 6. `Transformer`\n",
        "\n",
        "```python\n",
        "class Transformer(nn.Module): ...\n",
        "```\n",
        "\n",
        "مدل نهایی شامل:\n",
        "\n",
        "* Embedding اولیه با `VocabParallelEmbedding`\n",
        "* پشته‌ای از `TransformerBlock`s\n",
        "* Normalization نهایی\n",
        "* خروجی `ColumnParallelLinear` به اندازه‌ی واژگان\n",
        "\n",
        "همچنین:\n",
        "\n",
        "* `precompute_freqs_cis`: فرکانس‌های RoPE از قبل محاسبه می‌شود.\n",
        "* `@torch.inference_mode()`: نشان می‌دهد که این `forward()` برای inference است.\n",
        "* `mask`: برای causal attention استفاده می‌شود.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2qPUKSqaAux4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🦾 تمرین\n",
        "\n",
        "مقالات Transformer، BERT، GPT-2 و GPT-3 را مطالعه کنید و خلاصه‌ای از آنها بنویسید. نیازی به مطالعه خط به خط نیست؛ مرور ایده‌های اصلی کافی است. ؟\n",
        "\n",
        "---\n",
        "\n",
        "## 🔷 1. **Transformer (Vaswani et al., 2017) – \"Attention is All You Need\"**\n",
        "\n",
        "### 🧠 ایده‌ی اصلی:\n",
        "\n",
        "> کنار گذاشتن ساختارهای قدیمی مثل RNN و LSTM و استفاده صرف از **مکانیزم Attention** برای مدل‌سازی دنباله‌ها (sequences).\n",
        "\n",
        "### 📌 نکات کلیدی:\n",
        "\n",
        "* **Self-Attention:** هر توکن با سایر توکن‌ها تعامل دارد تا زمینهٔ معنایی را بهتر درک کند.\n",
        "* **Multi-Head Attention:** چندین attention موازی اجرا می‌شوند تا انواع مختلف روابط را بیاموزد.\n",
        "* **بدون RNN یا CNN**: کاملاً موازی‌سازی‌شده، سریع‌تر برای آموزش.\n",
        "* مدل شامل یک **encoder-decoder** با لایه‌های attention و feed-forward است.\n",
        "\n",
        "### 🌍 تأثیر:\n",
        "\n",
        "پایه‌گذار تمام مدل‌های بزرگ بعدی مثل BERT، GPT، T5 و LLaMA.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔷 2. **BERT (Devlin et al., 2018) – \"Bidirectional Encoder Representations from Transformers\"**\n",
        "\n",
        "### 🧠 ایده‌ی اصلی:\n",
        "\n",
        "> مدل زبانی **دوطرفه (bidirectional)** که کل جمله را (قبل و بعد) درک می‌کند؛ آموزش به‌صورت از‌پیش (pretraining) و بعد استفاده در وظایف مختلف.\n",
        "\n",
        "### 📌 نکات کلیدی:\n",
        "\n",
        "* از **encoder**های ترنسفورمر استفاده می‌کند (نه decoder).\n",
        "* **Masked Language Modeling (MLM):** در حین آموزش، برخی توکن‌ها پنهان می‌شوند و مدل باید آن‌ها را حدس بزند.\n",
        "* **Next Sentence Prediction (NSP):** تشخیص دهد که آیا دو جمله‌ی پشت‌سرهم به هم مربوط‌اند یا نه.\n",
        "* مناسب برای **درک متن**، نه تولید متن.\n",
        "\n",
        "### 🌍 تأثیر:\n",
        "\n",
        "انقلابی در **درک زبان** (مثلاً پاسخ‌ به سوال، طبقه‌بندی، ترجمه). الهام‌بخش RoBERTa، ALBERT، و DistilBERT.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔷 3. **GPT-2 (Radford et al., 2019) – \"Language Models are Unsupervised Multitask Learners\"**\n",
        "\n",
        "### 🧠 ایده‌ی اصلی:\n",
        "\n",
        "> یک مدل **زبان خودرگرسیو (Auto-regressive)** که فقط با پیش‌بینی کلمه بعدی، می‌تواند طیف وسیعی از وظایف زبانی را انجام دهد.\n",
        "\n",
        "### 📌 نکات کلیدی:\n",
        "\n",
        "* از **decoder-only Transformer** استفاده می‌کند (برخلاف BERT که encoder-only است).\n",
        "* فقط با **language modeling معمولی** آموزش می‌بیند (بدون ماسک یا NSP).\n",
        "* روی دیتاست عظیم **WebText** آموزش دیده.\n",
        "* هیچ fine-tuning خاصی انجام نمی‌شود؛ فقط از **prompting** استفاده می‌شود.\n",
        "\n",
        "### 🌍 تأثیر:\n",
        "\n",
        "نشان داد که یک مدل بزرگ زبانی می‌تواند بدون آموزش اختصاصی، **چند وظیفه مختلف را هم‌زمان انجام دهد** (zero-shot, one-shot, few-shot).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔷 4. **GPT-3 (Brown et al., 2020) – \"Language Models are Few-Shot Learners\"**\n",
        "\n",
        "### 🧠 ایده‌ی اصلی:\n",
        "\n",
        "> با **افزایش شدید اندازه مدل و داده‌ها**، می‌توان یادگیری few-shot را بهبود داد بدون نیاز به fine-tuning.\n",
        "\n",
        "### 📌 نکات کلیدی:\n",
        "\n",
        "* 175 میلیارد پارامتر (بزرگ‌ترین در زمان خود).\n",
        "* همان ساختار GPT-2 (decoder-only, auto-regressive) ولی بسیار بزرگ‌تر.\n",
        "* فقط با prompting می‌توان مدل را برای حل مسائلی مثل ترجمه، خلاصه‌سازی، کدنویسی، و... هدایت کرد.\n",
        "* دیگر نیاز به آموزش مجدد (fine-tuning) نیست — فقط مثال بده و مدل یاد می‌گیرد.\n",
        "\n",
        "### 🌍 تأثیر:\n",
        "\n",
        "نشان داد که مقیاس‌دهی عظیم می‌تواند به **یادگیری عمومی و منعطف** منجر شود؛ نقطهٔ عطف در ساخت مدل‌های مولد عمومی مثل ChatGPT و Copilot.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 مقایسه کوتاه:\n",
        "\n",
        "| مدل         | ساختار            | نوع یادگیری                 | هدف               | کاربرد اصلی               |\n",
        "| ----------- | ----------------- | --------------------------- | ----------------- | ------------------------- |\n",
        "| Transformer | Encoder + Decoder | Supervised (ترجمه)          | معماری پایه       | پایهٔ همه مدل‌های بعدی    |\n",
        "| BERT        | Encoder-only      | Pretraining + Fine-tuning   | درک زبان          | طبقه‌بندی، پاسخ‌به‌سوال   |\n",
        "| GPT-2       | Decoder-only      | فقط Pretraining             | تولید متن         | متن‌نویسی، multi-task     |\n",
        "| GPT-3       | Decoder-only      | فقط Pretraining + Prompting | few-shot learning | یادگیری منعطف، general AI |\n",
        "\n",
        "---\n",
        "\n",
        "اگر بخوای، می‌تونم این اطلاعات رو توی جدول PDF یا نمودار گرافیکی هم دربیارم، یا خلاصه انگلیسی‌شون رو هم ارائه بدم.\n"
      ],
      "metadata": {
        "id": "9n66c64aF1xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zi1rQlAgA3SN"
      }
    }
  ]
}