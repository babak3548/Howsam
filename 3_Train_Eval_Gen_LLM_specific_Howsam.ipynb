{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VrFCH9hpEyip",
        "3in1e9BksgIh",
        "d7Psbf21OXiW",
        "w_a3OXnSeV0z",
        "RwaY_YcgRayy",
        "RTql4Ftiunfr",
        "f8tPUUrtQ9Pj",
        "lAoYRsvKN_8l",
        "WTVewgG-m7nq",
        "ih6sV9ljndzW",
        "LCzTHf8iitEt",
        "W0QNbC0YPCKZ",
        "ACjVH6bBpAEV",
        "Tc_zTOcmOXir",
        "MLteBNsVNtsN",
        "o_5f69nwPtY2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uXkcYhkIxS-"
      },
      "source": [
        "#  <font color='#FFE15D'><b>💎 Train, Evaluate, and Generate Functions (LLM-specific) </b></font><font color='#FF0B55'><b>[Final]</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrFCH9hpEyip",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 🔴 **Environment Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3in1e9BksgIh",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## 🟠 Change the font size of the output cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_nYkVog8SUR",
        "outputId": "efb2679e-9f9f-43b1-b4a9-fd3474242b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salam Howsam!\n"
          ]
        }
      ],
      "source": [
        "print('Salam Howsam!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BmMM0EfKsSiO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "shell = get_ipython()\n",
        "\n",
        "def adjust_font_size():\n",
        "  display(HTML('''<style>\n",
        "    body {\n",
        "      font-size: 24px;\n",
        "    }\n",
        "  '''))\n",
        "\n",
        "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
        "  shell.events.register('pre_execute', adjust_font_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "10N1yUE88XRW",
        "outputId": "6769fcd2-1160-4669-8dc5-1a0b15349be2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salam Howsam!\n"
          ]
        }
      ],
      "source": [
        "print('Salam Howsam!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "d7Psbf21OXiW"
      },
      "source": [
        "## 🟠 `pip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jv21KFRNOXiX",
        "outputId": "bd024e28-465b-4852-8d02-5ab45bbd8059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.2\n"
          ]
        }
      ],
      "source": [
        "pip install  torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_a3OXnSeV0z",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 🔴 **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vhlVJEkJeTsV",
        "outputId": "2f264ad2-0aeb-491c-9860-aaa6684cfedf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from itertools import cycle\n",
        "from termcolor import colored\n",
        "from dataclasses import dataclass\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchmetrics import MeanMetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwaY_YcgRayy",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 🔴 **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5pmcazW7OXia",
        "outputId": "c3cf77d2-7e2b-4d34-9c7b-6e63d43fb0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def prepare_data(tokens, seq_len):\n",
        "    # Trim tokens so that total length is divisible by seq_len\n",
        "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
        "    tokens = tokens[:n_tokens]\n",
        "    # Reshape to 2D tensor\n",
        "    return tokens.view(-1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PpKbTUEIRayz",
        "outputId": "06a0325f-07f8-465c-ebd7-6cff92775312"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def num_trainable_params(model):\n",
        "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G6LDfZvmOLcI",
        "outputId": "98856e62-efa4-4355-dfe0-50d1c5a652dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Benchmarking function\n",
        "def calculate_time(model, x, num_runs=10):\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.time() - start) / num_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTql4Ftiunfr"
      },
      "source": [
        "# 🔴 **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xqqBMjGnz1IL",
        "outputId": "d6fe4923-bbbb-4add-d055-d5af7a909981"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class TinyStoriesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.seq_len = seq_len\n",
        "        self.data = prepare_data(data, seq_len+1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample.long()#[:-1], sample[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 **Model**"
      ],
      "metadata": {
        "id": "f8tPUUrtQ9Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Multi Head Attention"
      ],
      "metadata": {
        "id": "lAoYRsvKN_8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.head_size = self.n_embd // self.n_head\n",
        "\n",
        "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
        "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.c_proj.residual = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # QKV linear\n",
        "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
        "        # Scaled Dot Product Attention using pytorch\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        # Reshape and final projection\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "gnxYy1oIQcY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "33dfe7fd-8931-4abc-80f5-3d3f51f43d4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Feed Forward (MLP)"
      ],
      "metadata": {
        "id": "WTVewgG-m7nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.f_expnd = config.f_expnd\n",
        "\n",
        "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
        "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
        "        self.down_proj.residual = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(F.gelu(self.up_proj(x)))"
      ],
      "metadata": {
        "id": "BTcx4J5Lm66z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "05921b92-0690-408e-e342-575cfd2675d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 Decoder Block"
      ],
      "metadata": {
        "id": "ih6sV9ljndzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        # Multi Head Attention\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.mha = MultiHeadAttention(config)\n",
        "        # Feed Forward Neural Network\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gq1kw31av9DR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d20ebc47-a360-4e8c-ac5b-c482b05a9d78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🟠 GPT"
      ],
      "metadata": {
        "id": "LCzTHf8iitEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # Token embedding\n",
        "        self.wpe = nn.Embedding(config.max_seq_len, config.n_embd) # Position embedding\n",
        "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]) # Decoders\n",
        "        self.lnf = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Classifier\n",
        "        self.lm_head.weight = self.wte.weight # Weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'residual'):\n",
        "                std *= (2*self.config.n_layer)**-0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        # Token Embedding + Position Embedding\n",
        "        x = self.wte(idx) + self.wpe(torch.arange(T, device=idx.device))\n",
        "        # Decoders\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x)\n",
        "        # Classifier\n",
        "        x = self.lnf(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Lmrc034JwvSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2f703d1d-ef45-453d-c97e-00f12f9864ba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QNbC0YPCKZ"
      },
      "source": [
        "# 🔴 **Functions ⚙️**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACjVH6bBpAEV"
      },
      "source": [
        "## 🟠 Logger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logger class for saving and plotting training logs\n",
        "class Logger:\n",
        "    \"\"\"\n",
        "    Manages training history logging, saving to disk, and plotting learning curves.\n",
        "    \"\"\"\n",
        "    def __init__(self, log_dir='logs', run_name='default_run'):\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.run_name = run_name\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'valid_loss': [],\n",
        "            'best_loss_valid': float('inf'),\n",
        "            'seen_tokens': []\n",
        "        }\n",
        "\n",
        "    def log(self, train_loss, valid_loss, seen_tokens):\n",
        "        self.history['train_loss'].append(train_loss)\n",
        "        self.history['valid_loss'].append(valid_loss)\n",
        "        self.history['seen_tokens'].append(seen_tokens)\n",
        "\n",
        "    def save(self):\n",
        "        # Save history\n",
        "        file_path = os.path.join(self.log_dir, f'{self.run_name}.json')\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(self.history, f, indent=4)\n",
        "        # Save best model and optimizer\n",
        "        current_loss_valid = self.history['valid_loss'][-1]\n",
        "        if current_loss_valid < self.history['best_loss_valid']:\n",
        "            log = dict(model=model.state_dict(), optimizer=optimizer)\n",
        "            torch.save(log, f'{self.log_dir}/best-model.pt')\n",
        "            self.history['best_loss_valid'] = current_loss_valid\n",
        "            print(\"✅ Model Saved!\")\n",
        "\n",
        "    def plot(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.history['seen_tokens'], self.history['train_loss'], label='Train Loss')\n",
        "        plt.plot(self.history['seen_tokens'], self.history['valid_loss'], label='Valid Loss')\n",
        "        plt.xlabel('Seen Tokens')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Training Curve: {self.run_name}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.log_dir, f'{self.run_name}_curve.png'))\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yhLxBeKtreKx",
        "outputId": "2efcf5c7-58b0-4b09-8804-be7f0b1b1320"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc_zTOcmOXir"
      },
      "source": [
        "## 🟠 Train ➰"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer class to manage model training, evaluation and reporting\n",
        "class LLMTrainer:\n",
        "    \"\"\"\n",
        "    Trainer handles training loops, periodic evaluation, logging, and sample generation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, train_loader, valid_loader, tokenizer,\n",
        "                 loss_fn=F.cross_entropy, device='cuda',\n",
        "                 total_tokens=10_000_000, log_interval_tokens=1_000_000,\n",
        "                 generation=None):\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.device = device\n",
        "\n",
        "        self.seen_tokens = 0\n",
        "        self.total_tokens = total_tokens\n",
        "        self.token_eval_counter = 0\n",
        "        self.log_interval_tokens = log_interval_tokens\n",
        "\n",
        "        self.logger = Logger(log_dir='logs', run_name='gpt2_tinystories')\n",
        "        self._print_config_summary()\n",
        "\n",
        "        self.generation = generation\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Main training loop that stops when total token count is reached.\n",
        "        \"\"\"\n",
        "        # Initial evaluation before any training\n",
        "        initial_loss = self.evaluate()\n",
        "        self.logger.log(initial_loss, initial_loss, 0)\n",
        "        print(f\"👶 [Initial] Train Loss (Untrained Model): {initial_loss:.4f}\\n\")\n",
        "\n",
        "        loss_train = MeanMetric()\n",
        "        self.model.train()\n",
        "        train_iter = cycle(self.train_loader)\n",
        "\n",
        "        batches = 0\n",
        "        total_time_elapsed = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        with tqdm(total=self.total_tokens, desc=\"Training\", unit=\"t\") as pbar:\n",
        "            while self.seen_tokens < self.total_tokens:\n",
        "                # Get inputs\n",
        "                inputs = next(train_iter).to(self.device)\n",
        "                # Forward pass\n",
        "                logits = self.model(inputs[:, :-1])\n",
        "                # Calculate loss\n",
        "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                # Clip gradients\n",
        "                nn.utils.clip_grad.clip_grad_norm_(self.model.parameters(), max_norm=1.)\n",
        "                # Update model\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                # Calc running loss\n",
        "                loss_train.update(loss.item(), inputs.shape[0])\n",
        "\n",
        "                num_tokens_this_batch = inputs[:, :-1].numel()\n",
        "                self.seen_tokens += num_tokens_this_batch\n",
        "                self.token_eval_counter += num_tokens_this_batch\n",
        "                batches += 1\n",
        "                elapsed = time.time() - start_time\n",
        "                batches_per_sec = batches / elapsed\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    \"B/S\": f\"{batches_per_sec:.2f}\",\n",
        "                    \"Loss\": f\"{loss_train.compute().item():.4f}\",\n",
        "                    \"LR\": f\"{self.optimizer.param_groups[0]['lr']:.2e}\",\n",
        "                })\n",
        "                pbar.update(num_tokens_this_batch)\n",
        "\n",
        "                # Evaluate & Generate & Log\n",
        "                if (self.token_eval_counter >= self.log_interval_tokens) or (self.seen_tokens >= self.total_tokens):\n",
        "                    # Evaluate\n",
        "                    loss_valid = self.evaluate()\n",
        "                    print(f\"\\nValid Loss: {loss_valid:.4f}\")\n",
        "                    # Log\n",
        "                    self.logger.log(loss_train.compute().item(), loss_valid, self.seen_tokens)\n",
        "                    self.logger.save()\n",
        "                    # Generate\n",
        "                    if self.generation:\n",
        "                        self.generate()\n",
        "                    # Reset\n",
        "                    self.token_eval_counter = 0\n",
        "                    batches = 0\n",
        "                    start_time = time.time()\n",
        "\n",
        "        self.logger.plot()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluate model on validation set.\n",
        "        \"\"\"\n",
        "        loss_valid = MeanMetric()\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs in self.valid_loader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                logits = self.model(inputs[:, :-1])\n",
        "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
        "                loss_valid.update(loss.item(), inputs.shape[0])\n",
        "        return loss_valid.compute().item()\n",
        "\n",
        "    def generate(self):\n",
        "        \"\"\"\n",
        "        Generate and print text samples from the model.\n",
        "        \"\"\"\n",
        "        generated_texts = []\n",
        "        for prompt in self.generation.prompts:\n",
        "            gen_text = generate(\n",
        "                self.model, self.tokenizer, prompt,\n",
        "                n_rep=self.generation.n_rep,\n",
        "                max_seq_len=self.generation.max_seq_len,\n",
        "                T=self.generation.T, top_k=self.generation.top_k,\n",
        "                seed=self.generation.seed)\n",
        "            generated_texts.append(gen_text)\n",
        "        # TODO: Save\n",
        "        # Print\n",
        "        # print(150*'.')\n",
        "        # item = 0\n",
        "        # prompt0 = self.generation.prompts[item]\n",
        "        # for gen_text in generated_texts[item]:\n",
        "        #     print(colored(f\"\\n{prompt0}\", \"green\"), end='')\n",
        "        #     print(colored(f\"{gen_text[len(prompt0):]}\", \"cyan\"))\n",
        "        #     print(150*'.')\n",
        "        # print()\n",
        "        item = 0\n",
        "        prompt0 = self.generation.prompts[item]\n",
        "        gen_text0 = generated_texts[item][0]\n",
        "        print(colored(f\"\\n{prompt0}\", \"green\"), end='')\n",
        "        print(colored(f\"{gen_text0[len(prompt0):]}\", \"cyan\"))\n",
        "        print()\n",
        "\n",
        "    def _print_config_summary(self):\n",
        "        \"\"\"\n",
        "        Print a summary table of training configuration.\n",
        "        \"\"\"\n",
        "        table = PrettyTable()\n",
        "        table.title = \"Training Configuration Summary\"\n",
        "        table.field_names = [\"Component\", \"Details\"]\n",
        "        # Model\n",
        "        table.add_row([\"Model Type\", str(self.model.config).replace(\"Config\", \"\")])\n",
        "        # Optimizer\n",
        "        optimizer_name = self.optimizer.__class__.__name__\n",
        "        optimizer_params = ', '.join([f\"{k}={v}\" for k, v in self.optimizer.defaults.items() if k in [\"lr\", \"betas\", \"weight_decay\", \"fused\"]])\n",
        "        optimizer_display = f\"{optimizer_name}({optimizer_params})\"\n",
        "        table.add_row([\"Optimizer\", optimizer_display])\n",
        "        # Parameters\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        te_params = self.model.wte.weight.numel()\n",
        "        table.add_row([\"Total Parameters (Tr+TE)\", f\"{total_params:,} ({total_params-te_params:,}+{te_params:,})\"])\n",
        "\n",
        "        table.add_row([\"Loss Function\", self.loss_fn.__name__ if hasattr(self.loss_fn, '__name__') else str(self.loss_fn)])\n",
        "        table.add_row([\"Batch Shape\", f\"{self.train_loader.batch_size}x{self.train_loader.dataset[0].shape[-1]-1}\"])\n",
        "        table.add_row([\"Device\", self.device])\n",
        "        table.add_row([\"Max Tokens\", f\"{self.total_tokens:,}\"])\n",
        "        table.add_row([\"Log Interval Tokens\", f\"{self.log_interval_tokens:,}\"])\n",
        "        print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_6nuZ2JsE5d8",
        "outputId": "e94e485e-5446-43f8-89d8-308a12ba8a74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLteBNsVNtsN"
      },
      "source": [
        "## 🟠 Generate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, tokenizer, prompt, n_rep=5, max_seq_len=128, T=0.9, top_k=10, device='cuda', seed=42):\n",
        "    # Tokenize the prompt and convert it to a tensor on the specified device (e.g., GPU)\n",
        "    inputs = torch.tensor(tokenizer.encode(prompt).ids, dtype=torch.int, device=device)  # Shape: [T]\n",
        "\n",
        "    # Repeat the input prompt n_rep times to generate multiple sequences in parallel\n",
        "    inputs = inputs.unsqueeze(0).repeat(n_rep, 1)  # Shape: [B, T] where B = n_rep\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize a random number generator for sampling\n",
        "    sample_rng = torch.Generator(device=device)\n",
        "    sample_rng.manual_seed(seed)\n",
        "\n",
        "    # Disable gradient calculation for faster inference\n",
        "    with torch.no_grad():\n",
        "        # Continue generating tokens until reaching the maximum sequence length\n",
        "        while inputs.shape[-1] < max_seq_len:\n",
        "            # Forward pass: get logits from the model\n",
        "            logits = model(inputs)  # Shape: [B, T, vocab_size]\n",
        "\n",
        "            # Apply temperature scaling and softmax to get probabilities for the next token\n",
        "            probs = torch.softmax(logits[:, -1, :] / T, dim=-1)  # Shape: [B, vocab_size]\n",
        "\n",
        "            # Select the top_k tokens with the highest probabilities\n",
        "            topk_probs, topk_indices = torch.topk(probs, k=top_k, dim=-1)  # Shape: [B, top_k]\n",
        "\n",
        "            # Sample one token from the top_k candidates based on their probabilities\n",
        "            ids = torch.multinomial(topk_probs, 1, generator=sample_rng)  # Shape: [B, 1]\n",
        "\n",
        "            # Map the sampled indices back to the original token IDs\n",
        "            ids = torch.gather(topk_indices, -1, ids)  # Shape: [B, 1]\n",
        "\n",
        "            # Append the sampled tokens to the input sequence\n",
        "            inputs = torch.cat((inputs, ids), dim=-1)  # Shape: [B, T+1]\n",
        "\n",
        "    # Decode the generated sequences back into text\n",
        "    generated_text = tokenizer.decode_batch(inputs.tolist())\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "q8AHJrxFTfs2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "02ab86f8-22bf-4708-fc71-51acf082c312"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_chat_style(prompt, generated_text, tokenizer, delay=0.03):\n",
        "    \"\"\"\n",
        "    Display generated text in a token-by-token ChatGPT-like style:\n",
        "    - prompt in green\n",
        "    - generated continuation in blue\n",
        "    \"\"\"\n",
        "    for i, full_text in enumerate(generated_text):\n",
        "        print(colored(f\"\\n[Sample {i+1}]\", \"yellow\"))\n",
        "        input_ids = tokenizer.encode(prompt).ids\n",
        "        full_ids = tokenizer.encode(full_text).ids\n",
        "\n",
        "        # Split into prompt tokens and continuation\n",
        "        prompt_tokens = full_ids[:len(input_ids)]\n",
        "        continuation_tokens = full_ids[len(input_ids):]\n",
        "\n",
        "        # Decode tokens separately\n",
        "        prompt_text = tokenizer.decode(prompt_tokens)\n",
        "        cont_tokens_text = [tokenizer.decode([tid]) for tid in continuation_tokens]\n",
        "\n",
        "        # Print prompt in green\n",
        "        sys.stdout.write(colored(prompt_text, 'green'))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Print continuation token-by-token in blue\n",
        "        for token in cont_tokens_text:\n",
        "            sys.stdout.write(colored(token, 'cyan'))\n",
        "            sys.stdout.flush()\n",
        "            time.sleep(delay)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ItFI4ge5p_Tg",
        "outputId": "f6a600f3-9d14-4b67-b137-95807b736b95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_5f69nwPtY2"
      },
      "source": [
        "# 🔴 **Config**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    train_path: str\n",
        "    valid_path: str\n",
        "    tokenizer_path: str\n",
        "    batch_size: int = 32\n",
        "    seq_len: int = 128\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int = 50257\n",
        "    max_seq_len: int = 1024\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    f_expnd: int = 4\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OptimizerConfig:\n",
        "    lr: float = 3e-4\n",
        "    betas: tuple = (0.9, 0.95)\n",
        "    weight_decay: float = 0.1\n",
        "    fused: bool = True\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    seed: int = 42\n",
        "    device: str = 'cuda'\n",
        "    total_tokens: int = 100_000\n",
        "    log_interval_tokens: int = 50_000\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerationConfig:\n",
        "    prompts: list[str]\n",
        "    T: float = 0.9\n",
        "    max_seq_len: int = 128\n",
        "    top_k: int = 10\n",
        "    n_rep: int = 3\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MasterConfig:\n",
        "    data: DatasetConfig\n",
        "    model: GPTConfig\n",
        "    optimizer: OptimizerConfig\n",
        "    train: TrainConfig\n",
        "    generation: GenerationConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oIkx-mzbztIg",
        "outputId": "d60ee1af-39e5-462b-fb28-3eff3b75c502"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hafbAF356Xv"
      },
      "source": [
        "# 🔴 **Training Process 〽️**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hSXbwAUKRm7M",
        "outputId": "e1cd79c9-59f5-44d4-8638-7a0fe55c4ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = MasterConfig(\n",
        "\n",
        "    data=DatasetConfig(\n",
        "        train_path='/content/drive/MyDrive/temp/tokenized-train-samples_vocab-10k.pt',\n",
        "        valid_path='/content/drive/MyDrive/temp/tokenized-valid-samples_vocab-10k.pt',\n",
        "        tokenizer_path='/content/drive/MyDrive/temp/bpe-tokenizer_tinystories.json',\n",
        "        batch_size=192,\n",
        "        seq_len=128),\n",
        "\n",
        "    model=GPTConfig(\n",
        "        vocab_size=10_000,\n",
        "        max_seq_len=256,\n",
        "        n_layer=8,\n",
        "        n_head=16,\n",
        "        n_embd=128,\n",
        "        f_expnd=4),\n",
        "\n",
        "    optimizer=OptimizerConfig(\n",
        "        lr=6e-4,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=0.1,\n",
        "        fused=True),\n",
        "\n",
        "    train=TrainConfig(\n",
        "        seed=42,\n",
        "        device='cuda',\n",
        "        total_tokens=450_000_000,\n",
        "        log_interval_tokens=10_000_000),\n",
        "\n",
        "    generation=GenerationConfig(\n",
        "        prompts=['In last'],\n",
        "        T=0.9,\n",
        "        max_seq_len=128,\n",
        "        top_k=10,\n",
        "        n_rep=3,\n",
        "        seed=42)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8oeA0tN819pD",
        "outputId": "8b8f9442-cc2f-4d34-95ca-396384f7645b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a manual seed for reproducibility across runs\n",
        "torch.manual_seed(cfg.train.seed)\n",
        "\n",
        "# Load pre-tokenized training and validation token IDs from disk\n",
        "train_token_ids = torch.load(cfg.data.train_path)\n",
        "valid_token_ids = torch.load(cfg.data.valid_path)\n",
        "\n",
        "print(\"📊 Number of Tokens\")\n",
        "print(f\"🔹 Train: {len(train_token_ids):,} tokens\")\n",
        "print(f\"🔹 Valid: {len(valid_token_ids):,} tokens\")\n",
        "print()\n",
        "\n",
        "\n",
        "# Create dataset instances with fixed-length sequences\n",
        "train_set = TinyStoriesDataset(train_token_ids, cfg.data.seq_len)\n",
        "valid_set = TinyStoriesDataset(valid_token_ids, cfg.data.seq_len)\n",
        "\n",
        "\n",
        "# Create DataLoaders for batching and shuffling during training\n",
        "train_loader = DataLoader(train_set, batch_size=cfg.data.batch_size, shuffle=True, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=cfg.data.batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "print(f\"📊 Number of Batches\")\n",
        "print(f\"🔹 Train: {len(train_loader):,} batches\")\n",
        "print(f\"🔹 Valid: {len(valid_loader):,} batches\")"
      ],
      "metadata": {
        "id": "9FQ8l4rrK2nW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "10f58f39-79cf-4b6a-a3c0-114f302855d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Number of Tokens\n",
            "🔹 Train: 464,965,814 tokens\n",
            "🔹 Valid: 4,673,588 tokens\n",
            "\n",
            "📊 Number of Batches\n",
            "🔹 Train: 18,773 batches\n",
            "🔹 Valid: 189 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(cfg.data.tokenizer_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xpsiToA77Vex",
        "outputId": "2afcb77c-11ff-4b94-c3dc-03afc1598cde"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(cfg.model).to(cfg.train.device)\n",
        "print(model)\n",
        "print(f\"\\n📊 Number of Parameters: {num_trainable_params(model):.2f}M\")"
      ],
      "metadata": {
        "id": "i7zJv8yJEBLl",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "6e39df8b-514f-4ef4-f662-f77cba7078d4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (wte): Embedding(10000, 128)\n",
            "  (wpe): Embedding(256, 128)\n",
            "  (decoders): ModuleList(\n",
            "    (0-7): 8 x DecoderBlock(\n",
            "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mha): MultiHeadAttention(\n",
            "        (qkv_proj): Linear(in_features=128, out_features=384, bias=False)\n",
            "        (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "      )\n",
            "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): FeedForward(\n",
            "        (up_proj): Linear(in_features=128, out_features=512, bias=False)\n",
            "        (down_proj): Linear(in_features=512, out_features=128, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lnf): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=128, out_features=10000, bias=False)\n",
            ")\n",
            "\n",
            "📊 Number of Parameters: 2.89M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ketNqGoLsK2U",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b193ff67-1b90-4208-e5fa-0ad3aa5e27e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=cfg.optimizer.lr,\n",
        "    betas=cfg.optimizer.betas,\n",
        "    weight_decay=cfg.optimizer.weight_decay,\n",
        "    fused=cfg.optimizer.fused\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/temp/best-model.pt\", weights_only=False)  # یا weights_only=False برای جلوگیری از ارور\n",
        "\n",
        "# Load model weights\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "# Load optimizer state\n",
        "optimizer = checkpoint['optimizer']"
      ],
      "metadata": {
        "id": "hcW_XTaBLUBz",
        "outputId": "04135d9b-cc64-40ad-ab00-e6dfabb6a3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = LLMTrainer(\n",
        "    model, optimizer, train_loader, valid_loader, tokenizer,\n",
        "    total_tokens=cfg.train.total_tokens, log_interval_tokens=cfg.train.log_interval_tokens,\n",
        "    generation=cfg.generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "JMG2LHw6x4Rg",
        "outputId": "d955d8e3-3f20-4068-8c63-8ec9636a6eea"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------+\n",
            "|                                         Training Configuration Summary                                         |\n",
            "+--------------------------+-------------------------------------------------------------------------------------+\n",
            "|        Component         |                                       Details                                       |\n",
            "+--------------------------+-------------------------------------------------------------------------------------+\n",
            "|        Model Type        | GPT(vocab_size=10000, max_seq_len=256, n_layer=8, n_head=16, n_embd=128, f_expnd=4) |\n",
            "|        Optimizer         |          AdamW(lr=0.0006, betas=(0.9, 0.95), weight_decay=0.1, fused=True)          |\n",
            "| Total Parameters (Tr+TE) |                           2,889,984 (1,609,984+1,280,000)                           |\n",
            "|      Loss Function       |                                    cross_entropy                                    |\n",
            "|       Batch Shape        |                                       192x128                                       |\n",
            "|          Device          |                                         cuda                                        |\n",
            "|        Max Tokens        |                                     450,000,000                                     |\n",
            "|   Log Interval Tokens    |                                      10,000,000                                     |\n",
            "+--------------------------+-------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "4_N1nGVk7mtH",
        "outputId": "46ffe58c-0a45-413f-9b8a-07a6ab46f1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👶 [Initial] Train Loss (Untrained Model): 1.9843\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 3440640/450000000 [00:37<1:18:25, 94896.70t/s, B/S=3.73, Loss=1.9746, LR=6.00e-04]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcwx3XpZOXiw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "866f9806-6d9a-4cfd-a35e-0cce298950ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOVMvcCB7wjV"
      },
      "source": [
        "# 🔴 **Generate**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_colored_wrapped(prompt, generated, width=100):\n",
        "    \"\"\"\n",
        "    Print prompt and generated text with color and line wrapping, preserving paragraph breaks (\\n\\n).\n",
        "    \"\"\"\n",
        "    full_text = prompt + generated\n",
        "    paragraphs = full_text.split('\\n\\n')  # Split by paragraph\n",
        "\n",
        "    first = True\n",
        "    for para in paragraphs:\n",
        "        # Apply line wrapping per paragraph\n",
        "        lines = textwrap.wrap(para, width=width)\n",
        "\n",
        "        for line in lines:\n",
        "            if first:\n",
        "                # Print prompt in green and the rest in cyan\n",
        "                prompt_part = line[:len(prompt)]\n",
        "                gen_part = line[len(prompt):]\n",
        "                print(colored(prompt_part, \"green\") + colored(gen_part, \"cyan\"))\n",
        "                prompt = ''  # only on first line\n",
        "                first = False\n",
        "            else:\n",
        "                print(colored(line, \"cyan\"))\n",
        "\n",
        "        print()  # extra newline between paragraphs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bQ_TnnfC-k00",
        "outputId": "7a673372-dd8c-4eac-a0d5-6d41d4b95c83"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    'In last night',\n",
        "    'Once upon',\n",
        "    'Once upon a time',\n",
        "    'One day, a little boy named TimTommy was a smart 3 year old, much smarter',\n",
        "    'List of best crypto coin is']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Nn-rU3iy9d9k",
        "outputId": "a2b2bc34-a843-46bd-ea72-220875d175e1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for prompt in prompts:\n",
        "    # Generate n_rep samples\n",
        "    gen_text = generate(model, tokenizer, prompt, n_rep=3, max_seq_len=128, T=0.9, top_k=10)\n",
        "\n",
        "    # Print\n",
        "    print(100*\"=\")\n",
        "    for gtxt in gen_text:\n",
        "        prompt_len = len(prompt)\n",
        "        generated = gtxt[prompt_len:]\n",
        "        print_colored_wrapped(prompt, generated, width=100)\n",
        "        print(100*\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4OLk4HdN7zum",
        "outputId": "171f6499-0203-495c-9ed2-d53b647c9e58"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    body {\n",
              "      font-size: 24px;\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "In last night, the man was very sleepy. He had been sitting in bed all day until he fell asleep.\n",
            "\n",
            "The night night, he was so tired that he woke up. He closed his eyes, took out his blanket and\n",
            "started to lie down. He was so happy to be out in the dream again.\n",
            "\n",
            "The man smiled and hugged the night, before the night came. He felt a lot less scared and decided\n",
            "that he would remember the night he had a dream. But this night, he had the best dream that he had\n",
            "ever heard again.Once upon a time, there was a little girl named\n",
            "\n",
            "....................................................................................................\n",
            "In last night, they went to sleep. As the day went by, the sun was shining and the sky was blue.\n",
            "\n",
            "Mommy said, \"It's time to go home, Lily. The sun is shining, and the moon is shining brightly.\"\n",
            "\n",
            "Lily said, \"But it is getting dark, Daddy. We are going to visit our grandma.\"\n",
            "\n",
            "Mommy smiled and said, \"Let's go!\"\n",
            "\n",
            "Mommy and Lily walked and walked to the house. Daddy opened the door and they walked around in.\n",
            "Lily's mom was surprised, and said, \"That was a fun place, Lily!\"\n",
            "\n",
            "....................................................................................................\n",
            "In last night there was a little bird that was very sleepy. She was sad because she couldn't fly.\n",
            "The bird asked her to take a nap.\n",
            "\n",
            "The little bird said, \"I need to rest for a while!\"\n",
            "\n",
            "But the little bird was very hungry. So she asked the bird, \"Can you sleep now?\".\n",
            "\n",
            "The bird replied, \"Yes, I will sleep here.\"\n",
            "\n",
            "So she snuggled into bed and fell asleep. When the little bird was sleeping, the bird was happy that\n",
            "she had made the noise.The man had a friend. His name was Billy. One day,\n",
            "\n",
            "....................................................................................................\n",
            "====================================================================================================\n",
            "Once upon a time, there were two friends, Jack and Jill. They were both very happy, and often went\n",
            "on adventures together.\n",
            "\n",
            "One day, Jack was feeling very hungry. He saw a tasty pie and asked, \"Let's try it!\" Jill agreed and\n",
            "they both took a bite. As they both ate, they heard a loud noise.\n",
            "\n",
            "\"That's a pie!\" Jack said.\n",
            "\n",
            "Jill smiled, \"That's a nice apple!\" She was so scared she dropped the pie on Jack's arm.\n",
            "\n",
            "\"Let's go to the kitchen, we can share it,\" Jack said.\n",
            "\n",
            "....................................................................................................\n",
            "Once upon a time, there was a kind girl. She was very compassionate. She wanted to take care of her\n",
            "and family so she would never be alone.\n",
            "\n",
            "One day, she had an idea. She wanted to show everyone her love. She found the best place to spend\n",
            "time with the family in the town.\n",
            "\n",
            "The family all smiled at her and thanked her. They were very happy for her.\n",
            "\n",
            "The girl took her family's food. They gave her lots of delicious food, and everyone said she had so\n",
            "much joy.\n",
            "\n",
            "The end.Once upon a time there was a young girl.\n",
            "\n",
            "....................................................................................................\n",
            "Once upon a time, there lived a little girl called Sarah. Sarah had a lot of friends and was always\n",
            "having fun.\n",
            "\n",
            "One day, Sarah was playing in the park when she saw a big, scary lion. She was scared of the roar\n",
            "and ran away as fast as she could.\n",
            "\n",
            "The lion saw Sarah and asked why she was so scared. Sarah told the lion about her hiding spot and\n",
            "the fierce tiger said he was hungry.\n",
            "\n",
            "Sarah felt so scared, she had to run away as fast as she could!\n",
            "\n",
            "The fierce lion said, \"Don't be afraid, Sarah. I'm here\n",
            "\n",
            "....................................................................................................\n",
            "====================================================================================================\n",
            "Once upon a time, there was a big, scary snake. The snake was very big and it was very scary. It\n",
            "tried to escape from the snake, but the snake was too fast. It ate the snake all morning, and the\n",
            "snake felt very hungry.\n",
            "\n",
            "The snake ate the snake and its friends had a good time. They were very happy with their new\n",
            "friends.Once upon a time, in a small forest, there was a little bunny named Tim. Tim loved to hop\n",
            "very fast. One day, he saw a big, mean dog named Tom. Tom was very fearful, but he wanted to help\n",
            "the\n",
            "\n",
            "....................................................................................................\n",
            "Once upon a time there was a little bird. He lived in a big tree in a tree, looking for something to\n",
            "eat.\n",
            "\n",
            "One day, the little bird saw a big tree. The tree was very tall and had lots of leaves. The bird\n",
            "wanted to see what was on the tree, but it didn't.\n",
            "\n",
            "Just then, the sun came out and the little bird was so hungry. It asked the tree to help. The tree\n",
            "said, \"If you find some nuts to eat, you can make a yummy sandwich for me. But first, I have to find\n",
            "some.\"\n",
            "\n",
            "The little bird was\n",
            "\n",
            "....................................................................................................\n",
            "Once upon a time there was a little bird. He was so excited that he was flying over a big tree in\n",
            "the yard. He had never seen a nest before. He flew over to it and saw that it was very pretty and so\n",
            "tall.\n",
            "\n",
            "Suddenly, he heard someone shouting out loud. It was his friend, the fox. She looked around and saw\n",
            "him in the tree. \"Hey, why are you so grumpy?\" she asked.\n",
            "\n",
            "The fox replied, \"I am just looking at a bird that can fly.\"\n",
            "\n",
            "The little bird thought for a moment and said, \"That's a great idea\n",
            "\n",
            "....................................................................................................\n",
            "====================================================================================================\n",
            "One day, a little boy named TimTommy was a smart 3 year old, much smarter. He went to school every\n",
            "day and was very careful. One day, the teacher showed him all kinds of things and asked him to pick\n",
            "up the vegetables.\n",
            "\n",
            "\"What are you doing?\" asked Timmy.\n",
            "\n",
            "\"I found a cauliflower,\" said the teacher. \"It is very soft and crunchy. You can pick it up and take\n",
            "a bite.\"\n",
            "\n",
            "\"I'm so happy you want a taste!\" Timmy said.\n",
            "\n",
            "The teacher smiled. \"You must do something nice. Just take a taste it and find that\n",
            "\n",
            "....................................................................................................\n",
            "One day, a little boy named TimTommy was a smart 3 year old, much smarter than a little boy who\n",
            "loved to explore the world around him.\n",
            "\n",
            "He decided to take a walk outside one night and find out what it was like. He walked around and\n",
            "around and saw a big, green hill. He wanted to try and reach it, so he ran down and grabbed his\n",
            "shoes with both paws.\n",
            "\n",
            "Tommy saw a man sitting on a hill. The man looked sad and asked Timmy if he wanted to help. Tommy\n",
            "said he wanted to help the man. The man said no and the little man got very sad\n",
            "\n",
            "....................................................................................................\n",
            "One day, a little boy named TimTommy was a smart 3 year old, much smarter and more determined.\n",
            "\n",
            "One day, Timmy saw something strange. All the people were talking about to be the same. He saw that\n",
            "one of the people had a small, long, long, long beard and a big smile in his hair.\n",
            "\n",
            "\"Hey!\" said the man. \"I'm going to take the exam!\"\n",
            "\n",
            "Tim was scared. \"Please, can we go see the answers?\" he asked.\n",
            "\n",
            "The man smiled and said, \"No, you have to go first. I'll show you the\n",
            "\n",
            "....................................................................................................\n",
            "====================================================================================================\n",
            "List of best crypto coin is a big heart. It is red and shiny and has a red bow on a piece of paper.\n",
            "\n",
            "\"What is this?\" Lily asks her mom.\n",
            "\n",
            "\"It is a coin!\" her mom says. \"Do you love it?\"\n",
            "\n",
            "\"I love it!\" Lily says. \"It is sweet and shiny and fun!\"\n",
            "\n",
            "\"That is nice of you,\" her mom says. \"But first, let me keep it for dinner. And then you can share\n",
            "it with me. And you have to be nice to each other.\"\n",
            "\n",
            "Lily nods and smiles\n",
            "\n",
            "....................................................................................................\n",
            "List of best crypto coin is the coin. He does not want to play with Lily and Ben because he has no\n",
            "more coin. He has an idea.\n",
            "\n",
            "He goes to the man's shop and says, \"Hello, I have a coin. Do you want to buy a coin?\"\n",
            "\n",
            "The man looks at List. He does not have any money in his pocket. He thinks it is very nice. He says,\n",
            "\"OK, I will buy this coin. You can trade with this one. We will be your friends. You are my\n",
            "friends.\"\n",
            "\n",
            "He gives Listo a coin\n",
            "\n",
            "....................................................................................................\n",
            "List of best crypto coin is gone. She is so sad and sad that it had disappeared.\n",
            "\n",
            "The next day, L stoneo went to the park with her mom. She saw a big tree with a hole in it. She\n",
            "thought it would be safe and have a nice surprise for her.\n",
            "\n",
            "The next day, L stone had a special thing that she knew. It was a shiny thing she had found. She\n",
            "opened the hole and saw a beautiful butterfly in the sky. She was delighted and said, \"Mom, look! I\n",
            "found this butterfly!\"\n",
            "\n",
            "L stone opened and the fairy flew\n",
            "\n",
            "....................................................................................................\n"
          ]
        }
      ]
    }
  ]
}